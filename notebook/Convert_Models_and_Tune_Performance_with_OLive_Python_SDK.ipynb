{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Models and Tune Performance with OLive Python SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demos how to use OLive Python SDK to convert a model from other model framework to ONNX, and then tune performance for the converted ONNX model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Make sure you have [Docker](https://www.docker.com/get-started) installed and running. \n",
    "\n",
    "2) Install python dependency modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (3.2)\n",
      "Requirement already satisfied: netron in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: docker in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.25.1)\n",
      "Requirement already satisfied: onnx in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: pypiwin32==223; sys_platform == \"win32\" and python_version >= \"3.6\" in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from docker) (223)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from docker) (1.12.0)\n",
      "Requirement already satisfied: requests!=2.18.0,>=2.14.2 in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from docker) (2.21.0)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from docker) (0.56.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas) (1.17.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from onnx) (3.7.4.1)\n",
      "Requirement already satisfied: protobuf in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from onnx) (3.11.3)\n",
      "Requirement already satisfied: pywin32>=223 in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pypiwin32==223; sys_platform == \"win32\" and python_version >= \"3.6\"->docker) (224)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests!=2.18.0,>=2.14.2->docker) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests!=2.18.0,>=2.14.2->docker) (2019.3.9)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests!=2.18.0,>=2.14.2->docker) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests!=2.18.0,>=2.14.2->docker) (1.24.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from protobuf->onnx) (46.1.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 19.0.3, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install wget netron docker pandas onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Download OLive Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [..............................................................................] 19457 / 19457Downloaded onnxpipeline.py\n",
      "100% [................................................................................] 2694 / 2694Downloaded convert_test_data.py\n",
      "100% [..................................................................................] 709 / 709Downloaded config.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wget\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/microsoft/OLive/master/utils/\"\n",
    "sdk_files = [\"onnxpipeline.py\", \"convert_test_data.py\", \"config.py\"]\n",
    "sdk_dir = \"./python_sdk\"\n",
    "if not os.path.exists(sdk_dir):\n",
    "    os.makedirs(sdk_dir)\n",
    "\n",
    "for filename in sdk_files:\n",
    "    target_file = os.path.join(sdk_dir, filename)\n",
    "    if os.path.exists(target_file):\n",
    "        os.remove(target_file)\n",
    "    wget.download(url + filename, target_file)\n",
    "    print(\"Downloaded\", filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Pull the latest `mcr.microsoft.com/onnxruntime/onnx-converter` and `mcr.microsoft.com/onnxruntime/perf-tuning` docker images from MCR. This should take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\n",
      "latest: Pulling from onnxruntime/onnx-converter\n",
      "Digest: sha256:37479e01a7c4cd2e77c012f0fc3bb4e89e1b45b72c0d4bb22621286c5c02aa26\n",
      "Status: Image is up to date for mcr.microsoft.com/onnxruntime/onnx-converter:latest\n",
      "mcr.microsoft.com/onnxruntime/onnx-converter:latest\n",
      "Using default tag: latest\n",
      "latest: Pulling from onnxruntime/perf-tuning\n",
      "Digest: sha256:8003f5ecd2e11c2fdad31610294982404954570c4ced0caed7b9b6f268e9388f\n",
      "Status: Image is up to date for mcr.microsoft.com/onnxruntime/perf-tuning:latest\n",
      "mcr.microsoft.com/onnxruntime/perf-tuning:latest\n"
     ]
    }
   ],
   "source": [
    "# pull onnx-converter and perf-tuning docker images from mcr\n",
    "!docker pull mcr.microsoft.com/onnxruntime/onnx-converter\n",
    "!docker pull mcr.microsoft.com/onnxruntime/perf-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you're ready to use OLive to convert model and tune performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convert Model To ONNX\n",
    "\n",
    "Convert the model of your choice to ONNX. The Python scripts will run OLive `onnx-converter` Docker image in the backend. It first converts the model, then tries to generate input test data if none provided, and finally runs the converted model alongside the original model for correctness check. For more information on the `onnx-converter` Docker image, please see [onnx-converter README.md](https://github.com/microsoft/OLive/blob/master/docker-images/onnx-converter/README.md)\n",
    "\n",
    "### Prepare model and test data files\n",
    "\n",
    "First you'll need to prepare your model and test data files. Supported model frameworks are - cntk, coreml, keras, scikit-learn, tensorflow and pytorch. \n",
    "\n",
    "For `onnx-converter`, test data are used for a quick verification of correctness for the converted model. We strongly recommend you provide your own test input and output files. However if no input files are provided, OLive will randomly generate dummy inputs for you if possible. Only one test data set is needed, but feel free to put in more data sets as they will be available for the next `perf-tuning` step to use. \n",
    "\n",
    "You can put your test data in one of - \n",
    "\n",
    "  1) Your input folder with your model from another framework.\n",
    "  \n",
    "  2) Your output folder created in advance to hold your converted ONNX model.\n",
    "  \n",
    "  3) Any other location. Need to specify the path with `--test_data_path` parameter to `onnx-converter`.\n",
    "  \n",
    "The best practice to put your input model file(s) and test data(optional) is **2)**. By putting test_data_sets in the \"output\" folder instead of the \"input\" folder, this approach avoids copying files in the backend. The folder structure will be as below:\n",
    "\n",
    "    - your_input_folder\n",
    "       - model_file(s)\n",
    "    - your_output_folder_to_hold_onnx_file\n",
    "       - test_data_set_0\n",
    "           - input_0.pb\n",
    "           - ...\n",
    "           - output_0.pb\n",
    "           - ...\n",
    "       - test_data_set_1\n",
    "           - ...\n",
    "       ...\n",
    "       - (your .onnx file after running \"onnx-converter\")\n",
    "\n",
    "\n",
    "\n",
    "#### [OPTIONAL] Convert Test Data to ONNX pb \n",
    "\n",
    "ONNX .pb files are expected for test data. If you're more familiar with pickle files, we provide a convenient script to convert your pickle data to pb. Dump your input data to a single pickle file in the following dict format - \n",
    "\n",
    "    {\n",
    "        \"input_name_0\": input_data_0,\n",
    "        \"input_name_1\": input_data_1, \n",
    "        ...\n",
    "    }\n",
    "    \n",
    "or if dumping output data - \n",
    "\n",
    "    {\n",
    "        \"output_name_0\": output_data_0,\n",
    "        \"output_name_1\": output_data_1, \n",
    "        ...\n",
    "    }\n",
    "\n",
    "Then use the [convert_test_data.py](https://github.com/microsoft/OLive/blob/master/utils/convert_test_data.py) to convert your pickle file to pb files.\n",
    "\n",
    "Run convert_test_data.py to convert your pickle file. This script will read your pickle file and dump the data to a folder named \"test_data_set_0\" by default. Note that ONNX naming convention for test data folder is \"test_data_*\". Make sure to pass `--output_folder` with a folder name starting with `test_data_`. \n",
    "\n",
    "If `--is_input=True`, data will be generated to `input_*.pb`s. Set `--is_input` to false if you'd like to generate output pbs, in which data will be generated to `output_*.pb`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the file specified.\n"
     ]
    }
   ],
   "source": [
    "!./python_sdk/convert_test_data.py <your_input_pickle_file> --output_folder <output_folder> --is_input=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you're ready to use OLive to convert model and tune performance. In this tutorial, we'll use [MNIST model from ONNX model zoo](https://github.com/onnx/models/tree/master/vision/classification/mnist) as an example to demo the OLive pipeline. Below are some code to download the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model directory  test_models\n",
      "Model successfully downloaded and extracted in  test_models\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Download and store the model to the desire directory. Modify these lines to point to your local model directory \n",
    "model_url = \"https://onnxzoo.blob.core.windows.net/models/opset_8/mnist/mnist.tar.gz\"\n",
    "model_dir = \"test_models\"\n",
    "absolute_model_dir = os.path.abspath(model_dir)\n",
    "if not os.path.exists(model_dir):\n",
    "    print(\"Creating model directory \", model_dir)\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "import tarfile\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "file_tmp = urlretrieve(model_url, filename=None)[0]\n",
    "\n",
    "tar = tarfile.open(file_tmp)\n",
    "tar.extractall(model_dir)\n",
    "print(\"Model successfully downloaded and extracted in \", model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate onnxpipeline\n",
    "\n",
    "After the model files are stored in a directory, we can pass the directory into onnxpipeline to start the model conversion. The parameters for onnxpipeline are listed below. \n",
    "\n",
    "    (1) local_directory: string\n",
    "        Required. The path of local directory where would be mounted to the docker. All operations will be executed from this path.\n",
    "\n",
    "    (2) mount_path: string\n",
    "        Optional. The path where the local_directory will be mounted in the docker. Default is \"/mnt/model\".\n",
    "\n",
    "    (3) print_logs: boolean\n",
    "        Optional. Whether print the logs from the docker. Default is True.\n",
    "\n",
    "    (4) convert_directory: string\n",
    "        Optional. The directory path for converting model. Default is test/.    \n",
    "\n",
    "    (5) convert_name: string\n",
    "        Optional. The model name for converting model. Default is model.onnx.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------config----------------\n",
      "           Container information: <docker.client.DockerClient object at 0x00000241A77A6DA0>\n",
      " Local directory path for volume: C:\\Users\\ziyl.NORTHAMERICA\\OLive\\notebook/test_models\\mnist\n",
      "Volume directory path in dockers: /mnt/model\n",
      "                     Result path: result\n",
      "        Converted directory path: test\n",
      "        Converted model filename: model.onnx\n",
      "            Converted model path: test/model.onnx\n",
      "        Print logs in the docker: True\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('./python_sdk')\n",
    "import onnxpipeline\n",
    "\n",
    "# Initiate ONNX pipeline with your model path\n",
    "pipeline = onnxpipeline.Pipeline(os.path.join(model_dir, 'mnist'))\n",
    "\n",
    "# Check the configs\n",
    "pipeline.config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Model From Various Frameworks to ONNX Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLive is capable of converting models from most major frameworks to ONNX. Different frameworks may require different inputs. Check the commented code below to see what parameters are required for each supported model framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:root:TensorFlow version 1.15.0 detected. Last version known to be fully compatible is 1.14.0 .\n",
      "\n",
      "\n",
      "\n",
      "-------------\n",
      "\n",
      "Model Conversion\n",
      "\n",
      "\n",
      "\n",
      "Input model is already ONNX model. Skipping conversion.\n",
      "\n",
      "\n",
      "\n",
      "-------------\n",
      "\n",
      "MODEL INPUT GENERATION(if needed)\n",
      "\n",
      "\n",
      "\n",
      "Test data .pb files found under /mnt/model/test_data_set_0. \n",
      "\n",
      "copying /mnt/model/test_data_set_0 to /mnt/model/test/test_data_set_0\n",
      "\n",
      "Test data .pb files found under /mnt/model/test_data_set_1. \n",
      "\n",
      "copying /mnt/model/test_data_set_1 to /mnt/model/test/test_data_set_1\n",
      "\n",
      "Test data .pb files found under /mnt/model/test_data_set_2. \n",
      "\n",
      "copying /mnt/model/test_data_set_2 to /mnt/model/test/test_data_set_2\n",
      "\n",
      "Test data .pb files already exist. Skipping dummy input generation. \n",
      "\n",
      "\n",
      "\n",
      "-------------\n",
      "\n",
      "MODEL CORRECTNESS VERIFICATION\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Check the ONNX model for validity \n",
      "\n",
      "The ONNX model is valid.\n",
      "\n",
      "\n",
      "\n",
      "The original model is already onnx. Skipping correctness test. \n",
      "\n",
      "\n",
      "\n",
      "-------------\n",
      "\n",
      "MODEL CONVERSION SUMMARY (.json file generated at /mnt/model/test/output.json )\n",
      "\n",
      "\n",
      "\n",
      "{'conversion_status': 'SUCCESS',\n",
      "\n",
      " 'correctness_verified': 'SKIPPED',\n",
      "\n",
      " 'error_message': '',\n",
      "\n",
      " 'input_folder': '/mnt/model/test/test_data_set_0',\n",
      "\n",
      " 'output_onnx_path': '/mnt/model/test/model.onnx'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = pipeline.convert_model(model='model.onnx', model_type='onnx')\n",
    "\n",
    "# test tensorflow savedmodel (no need to specify \"model=\" as it takes a directory as input)\n",
    "# model = pipeline.convert_model(model_type='tensorflow')\n",
    "\n",
    "# test tensorflow savedgraph and checkpoints\n",
    "# model = pipeline.convert_model(model_type='tensorflow', model='saved_model.pb/meta', \n",
    "#                                model_input_names=\"input_name_1,input_name_2...\", \n",
    "#                                model_output_names=\"output_name_1,output_name_2...\")\n",
    "\n",
    "# test pytorch\n",
    "# model = pipeline.convert_model(model_type='pytorch', model='saved_model.pb', model_input_shapes='(1,3,224,224)')\n",
    "\n",
    "# test cntk\n",
    "# model = pipeline.convert_model(model_type='cntk', model='ResNet50_ImageNet_Caffe.model')\n",
    "\n",
    "# test keras\n",
    "# model = pipeline.convert_model(model_type='keras', model='keras_Average_ImageNet.keras')\n",
    "\n",
    "# test sklearn\n",
    "# model = pipeline.convert_model(model_type='scikit-learn', model='sklearn_svc.joblib', initial_types=(\"float_input\", \"FloatTensorType([1,4])\"))\n",
    "\n",
    "# test caffe\n",
    "# model = pipeline.convert_model(model_type='caffe', model='bvlc_alexnet.caffemodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Performance Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have an ONNX model and its test data, you can run performance tuning using OLive Python SDK `pipeline.perf_tuning` with your desired options. A complete list of available options are listed below. \n",
    "\n",
    "    (1) model: string\n",
    "\n",
    "        Required. The path of the model that wants to be performed.\n",
    "\n",
    "    (2) result: string\n",
    "\n",
    "        Optional. The path of the result.\n",
    "\n",
    "    (3) config: string (choices=[\"Debug\", \"MinSizeRel\", \"Release\", \"RelWithDebInfo\"])\n",
    "\n",
    "        Optional. Configuration to run. Default is \"RelWithDebInfo\".\n",
    "\n",
    "    (4) test_mode: string (choices=[\"duration\", \"times\"])\n",
    "\n",
    "        Optional. Specifies the test mode. Value could be 'duration' or 'times'. Default is \"times\".\n",
    "\n",
    "    (5) execution_provider: string (choices=[\"cpu\", \"cuda\", \"mkldnn\"])\n",
    "\n",
    "        Optional. help=\"Specifies the provider 'cpu','cuda','mkldnn'. Default is ''.\n",
    "\n",
    "    (6) repeated_times: integer\n",
    "\n",
    "        Optional. Specifies the repeated times if running in 'times' test mode. Default:20.\n",
    "\n",
    "    (7) duration_times: integer\n",
    "\n",
    "        Optional. Specifies the seconds to run for 'duration' mode. Default:10.\n",
    "\n",
    "    (8) parallel: boolean\n",
    "\n",
    "        Optional. Use parallel executor, default (without -x): sequential executor.\n",
    "\n",
    "    (9) intra_op_num_threads: integer\n",
    "\n",
    "        Optional. Sets the number of threads used to parallelize the execution within nodes, A value of 0 means ORT will pick a default. Must >=0.\n",
    "\n",
    "    (10) inter_op_num_threads: integer\n",
    "\n",
    "        Optional. Sets the number of threads used to parallelize the execution of the graph (across nodes), A value of 0 means ORT will pick a default. Must >=0.\n",
    "\n",
    "    (11) top_n: integer\n",
    "\n",
    "        Optional. Show percentiles for top n runs. Default:5.\n",
    "\n",
    "    (12) runtime: boolean\n",
    "\n",
    "        Optional. Use this boolean flag to enable GPU if you have one.\n",
    "\n",
    "    (13) input_json: string\n",
    "\n",
    "        Optional. Use JSON file as input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.0043349 s\n",
      "\n",
      "Total inference time cost:0.0011741 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.058705 ms\n",
      "\n",
      "Total inference run time:0.0011951 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.0044639 s\n",
      "\n",
      "Total inference time cost:0.0079206 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.39603 ms\n",
      "\n",
      "Total inference run time:0.0079622 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.0047126 s\n",
      "\n",
      "Total inference time cost:0.0087747 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.438735 ms\n",
      "\n",
      "Total inference run time:0.0088293 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.0107289 s\n",
      "\n",
      "Total inference time cost:0.0020869 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.104345 ms\n",
      "\n",
      "Total inference run time:0.0021279 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.0050273 s\n",
      "\n",
      "Total inference time cost:0.0101048 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.50524 ms\n",
      "\n",
      "Total inference run time:0.0102163 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.0043336 s\n",
      "\n",
      "Total inference time cost:0.0011798 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.05899 ms\n",
      "\n",
      "Total inference run time:0.0011994 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.0042215 s\n",
      "\n",
      "Total inference time cost:0.0011236 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.05618 ms\n",
      "\n",
      "Total inference run time:0.0011429 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.0038024 s\n",
      "\n",
      "Total inference time cost:0.0015595 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.077975 ms\n",
      "\n",
      "Total inference run time:0.0015871 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.0054405 s\n",
      "\n",
      "Total inference time cost:0.0014077 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.070385 ms\n",
      "\n",
      "Total inference run time:0.0014319 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.0094299 s\n",
      "\n",
      "Total inference time cost:0.0015665 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.078325 ms\n",
      "\n",
      "Total inference run time:0.0015998 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.0076793 s\n",
      "\n",
      "Total inference time cost:0.0036582 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.18291 ms\n",
      "\n",
      "Total inference run time:0.0037059 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.0062716 s\n",
      "\n",
      "Total inference time cost:0.0095244 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.47622 ms\n",
      "\n",
      "Total inference run time:0.0095713 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.0059189 s\n",
      "\n",
      "Total inference time cost:0.0017381 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.086905 ms\n",
      "\n",
      "Total inference run time:0.0017632 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.004885 s\n",
      "\n",
      "Total inference time cost:0.001177 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.05885 ms\n",
      "\n",
      "Total inference run time:0.0011943 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.0043721 s\n",
      "\n",
      "Total inference time cost:0.0081299 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.406495 ms\n",
      "\n",
      "Total inference run time:0.0081599 s\n",
      "\n",
      "Setting intra_op_num_threads to 0\n",
      "\n",
      "Session creation time cost:0.0064409 s\n",
      "\n",
      "Total inference time cost:0.0035506 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.17753 ms\n",
      "\n",
      "Total inference run time:0.0035902 s\n",
      "\n",
      "/home/ziyl/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:107 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] /home/ziyl/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:101 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] CUDA failure 35: CUDA driver version is insufficient for CUDA runtime version ; GPU=32536 ; hostname=608b280e32f1 ; expr=cudaSetDevice(device_id_); \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Setting intra_op_num_threads to 0\n",
      "\n",
      "/home/ziyl/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:107 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] /home/ziyl/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:101 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] CUDA failure 35: CUDA driver version is insufficient for CUDA runtime version ; GPU=32756 ; hostname=608b280e32f1 ; expr=cudaSetDevice(device_id_); \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Setting intra_op_num_threads to 0\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "[00:02:00] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "\n",
      "[00:02:00] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "\n",
      "[00:02:00] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=32\n",
      "\n",
      "Session creation time cost:0.101422 s\n",
      "\n",
      "Total inference time cost:0.0015598 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.07799 ms\n",
      "\n",
      "Total inference run time:0.0015801 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "[00:02:00] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "\n",
      "[00:02:00] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "\n",
      "[00:02:00] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=32\n",
      "\n",
      "Session creation time cost:0.0984962 s\n",
      "\n",
      "Total inference time cost:0.0019713 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.098565 ms\n",
      "\n",
      "Total inference run time:0.002 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "[00:02:01] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "\n",
      "[00:02:01] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "\n",
      "[00:02:01] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=32\n",
      "\n",
      "Session creation time cost:0.105744 s\n",
      "\n",
      "Total inference time cost:0.0014944 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.07472 ms\n",
      "\n",
      "Total inference run time:0.0015146 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "[00:02:01] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "\n",
      "[00:02:01] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "\n",
      "[00:02:01] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=32\n",
      "\n",
      "Session creation time cost:0.0930364 s\n",
      "\n",
      "Total inference time cost:0.0018998 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.09499 ms\n",
      "\n",
      "Total inference run time:0.0019245 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "[00:02:02] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "\n",
      "[00:02:02] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "\n",
      "[00:02:02] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=32\n",
      "\n",
      "Session creation time cost:0.0946809 s\n",
      "\n",
      "Total inference time cost:0.0018531 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.092655 ms\n",
      "\n",
      "Total inference run time:0.0018789 s\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.0065263 s\n",
      "\n",
      "Total inference time cost:0.0184397 s\n",
      "\n",
      "Total inference requests:200\n",
      "\n",
      "Average inference time cost:0.0921985 ms\n",
      "\n",
      "Total inference run time:0.0187502 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.0059525 s\n",
      "\n",
      "Total inference time cost:0.0110159 s\n",
      "\n",
      "Total inference requests:200\n",
      "\n",
      "Average inference time cost:0.0550795 ms\n",
      "\n",
      "Total inference run time:0.011183 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.0047322 s\n",
      "\n",
      "Total inference time cost:0.0141941 s\n",
      "\n",
      "Total inference requests:200\n",
      "\n",
      "Average inference time cost:0.0709705 ms\n",
      "\n",
      "Total inference run time:0.0144059 s\n",
      "\n",
      "Setting intra_op_num_threads to 0\n",
      "\n",
      "Session creation time cost:0.0041639 s\n",
      "\n",
      "Total inference time cost:0.0158238 s\n",
      "\n",
      "Total inference requests:200\n",
      "\n",
      "Average inference time cost:0.079119 ms\n",
      "\n",
      "Total inference run time:0.0160525 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "[00:02:04] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "\n",
      "[00:02:04] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "\n",
      "[00:02:04] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=32\n",
      "\n",
      "Session creation time cost:0.105746 s\n",
      "\n",
      "Total inference time cost:0.0156281 s\n",
      "\n",
      "Total inference requests:200\n",
      "\n",
      "Average inference time cost:0.0781405 ms\n",
      "\n",
      "Total inference run time:0.0158095 s\n",
      "\n",
      "Cores:  2\n",
      "\n",
      "No GPU found on current device. Cuda and TensorRT performance tuning might not be available. \n",
      "\n",
      "providers  ['cpu_openmp', 'mklml', 'dnnl', 'cpu', 'tensorrt', 'ngraph', 'cuda', 'nuphar']\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "OMP_NUM_THREADS=2\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/ec21be9e-79fb-4ff0-8324-9905a2d9dc9a\n",
      "\n",
      "cpu_openmp_2_OMP_threads_OMP_WAIT_POLICY_active 0.058705\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "\n",
      "OMP_NUM_THREADS=2\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/526fdfe3-b2a5-4e24-bcc4-f4421b92c5a2\n",
      "\n",
      "cpu_openmp_2_OMP_threads_OMP_WAIT_POLICY_passive 0.39603\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/56899054-de61-4177-96c7-e69145456e33\n",
      "\n",
      "cpu_openmp 0.438735\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/50a52672-739d-4f15-8a15-99683a824ee1\n",
      "\n",
      "cpu_openmp_OMP_WAIT_POLICY_active 0.10434500000000001\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/a3687251-9f81-43d7-906b-a45c00b59fe0\n",
      "\n",
      "cpu_openmp_OMP_WAIT_POLICY_passive 0.50524\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "OMP_NUM_THREADS=2\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/4bdc2452-ff7a-4bcf-b792-a1bcbe1b686c\n",
      "\n",
      "mklml_2_OMP_threads_OMP_WAIT_POLICY_active 0.05899\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "\n",
      "OMP_NUM_THREADS=2\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/4e6312ad-23c2-4a91-a3b7-f8a83f492a2f\n",
      "\n",
      "mklml_2_OMP_threads_OMP_WAIT_POLICY_passive 0.05618\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/a5052847-4888-4697-be4d-789f572b1609\n",
      "\n",
      "mklml 0.077975\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/5a48fa93-b3f0-4f44-8de7-cae7a7feb6d3\n",
      "\n",
      "mklml_OMP_WAIT_POLICY_active 0.070385\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/b83a7f9c-92c5-4c41-9af7-9ef001bd6bd2\n",
      "\n",
      "mklml_OMP_WAIT_POLICY_passive 0.078325\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "OMP_NUM_THREADS=2\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/e607b9bb-f5b6-49b1-a6ff-48a8d638ba7d\n",
      "\n",
      "dnnl_2_OMP_threads_OMP_WAIT_POLICY_active 0.18291\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "\n",
      "OMP_NUM_THREADS=2\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/03d9ff2f-ceb8-4b04-9a04-5eab53e2263f\n",
      "\n",
      "dnnl_2_OMP_threads_OMP_WAIT_POLICY_passive 0.47622000000000003\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/20d98b51-f77b-483e-9cdc-7f1a69fdd4c3\n",
      "\n",
      "dnnl 0.086905\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/25ab088b-27bd-4adb-8aae-4f3fc2dbf827\n",
      "\n",
      "dnnl_OMP_WAIT_POLICY_active 0.05885\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/9bbb667f-3662-481a-b4ad-b6e8b54a04c2\n",
      "\n",
      "dnnl_OMP_WAIT_POLICY_passive 0.406495\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e cpu -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/906cc126-5b2d-48a9-be67-88260fa6dcf4\n",
      "\n",
      "cpu 0.17753\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e tensorrt -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/48f224f8-ae25-4a40-891a-87f7df0e600e\n",
      "\n",
      "tensorrt None\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "OMP_NUM_THREADS=2\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/ngraph/onnxruntime_perf_test -e ngraph -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/7459ec70-f76d-4d90-80ac-3c9c8c790b36\n",
      "\n",
      "ngraph_2_OMP_threads_OMP_WAIT_POLICY_active None\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "\n",
      "OMP_NUM_THREADS=2\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/ngraph/onnxruntime_perf_test -e ngraph -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/7965f769-b6b0-423f-bbe4-dfefae999765\n",
      "\n",
      "ngraph_2_OMP_threads_OMP_WAIT_POLICY_passive None\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/ngraph/onnxruntime_perf_test -e ngraph -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/2bcb400b-aad9-490c-b315-8bf74326af89\n",
      "\n",
      "ngraph None\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/ngraph/onnxruntime_perf_test -e ngraph -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/bf47bbd7-7fa3-4003-a0ea-381d5a468af3\n",
      "\n",
      "ngraph_OMP_WAIT_POLICY_active None\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/ngraph/onnxruntime_perf_test -e ngraph -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/a9ae2705-d74e-43d7-b319-24feb42632bf\n",
      "\n",
      "ngraph_OMP_WAIT_POLICY_passive None\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e cuda -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/070e2e07-63f8-408b-8f2f-4340c01e49f5\n",
      "\n",
      "cuda None\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "OMP_NUM_THREADS=2\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -e nuphar -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/ba34d976-925a-4834-ad31-53c8b5f23ca7\n",
      "\n",
      "nuphar_2_OMP_threads_OMP_WAIT_POLICY_active 0.07799\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "\n",
      "OMP_NUM_THREADS=2\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -e nuphar -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/607c2e13-306b-4614-be30-41d3ee9dc740\n",
      "\n",
      "nuphar_2_OMP_threads_OMP_WAIT_POLICY_passive 0.098565\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -e nuphar -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/83289938-4b5b-4084-8f99-44a459413a0c\n",
      "\n",
      "nuphar 0.07472000000000001\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -e nuphar -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/c4d8808c-a982-49bd-80f7-edff7c3acf3b\n",
      "\n",
      "nuphar_OMP_WAIT_POLICY_active 0.09498999999999999\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -e nuphar -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/38e79931-e6d3-47c9-85c2-1f1a0e9b402c\n",
      "\n",
      "nuphar_OMP_WAIT_POLICY_passive 0.092655\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "OMP_NUM_THREADS=2\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -x 1 -o 99 -m times -r 200 /mnt/model/test/model.onnx /mnt/model/result/128bf87f-934e-4094-a25e-c2f0d8eca694\n",
      "\n",
      "cpu_openmp_2_OMP_threads_OMP_WAIT_POLICY_active 0.092198\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "\n",
      "OMP_NUM_THREADS=2\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -x 1 -o 99 -m times -r 200 /mnt/model/test/model.onnx /mnt/model/result/e8758038-4ef3-47c2-990c-804afaa7d09e\n",
      "\n",
      "mklml_2_OMP_threads_OMP_WAIT_POLICY_passive 0.055078999999999996\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -x 1 -o 99 -m times -r 200 /mnt/model/test/model.onnx /mnt/model/result/2af2cdbc-8ceb-4836-ac12-0203c629ce0a\n",
      "\n",
      "dnnl_OMP_WAIT_POLICY_active 0.07097\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e cpu -o 99 -m times -r 200 /mnt/model/test/model.onnx /mnt/model/result/0ea0f124-48a1-4871-9520-06f8f68beec0\n",
      "\n",
      "cpu 0.079119\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -e nuphar -x 1 -o 99 -m times -r 200 /mnt/model/test/model.onnx /mnt/model/result/e3b0c170-4d00-4f84-bfbe-20cc55120bfd\n",
      "\n",
      "nuphar 0.07813999999999999\n",
      "\n",
      "\n",
      "\n",
      "Results:\n",
      "\n",
      "mklml_2_OMP_threads_OMP_WAIT_POLICY_passive 0.055078999999999996 ms\n",
      "\n",
      "mklml_2_OMP_threads_OMP_WAIT_POLICY_active 0.05899 ms\n",
      "\n",
      "mklml_OMP_WAIT_POLICY_active 0.070385 ms\n",
      "\n",
      "dnnl_OMP_WAIT_POLICY_active 0.07097 ms\n",
      "\n",
      "dnnl 0.086905 ms\n",
      "\n",
      "dnnl_2_OMP_threads_OMP_WAIT_POLICY_active 0.18291 ms\n",
      "\n",
      "nuphar 0.07813999999999999 ms\n",
      "\n",
      "nuphar_2_OMP_threads_OMP_WAIT_POLICY_active 0.07799 ms\n",
      "\n",
      "nuphar_OMP_WAIT_POLICY_passive 0.092655 ms\n",
      "\n",
      "cpu 0.079119 ms\n",
      "\n",
      "cpu_openmp_2_OMP_threads_OMP_WAIT_POLICY_active 0.092198 ms\n",
      "\n",
      "cpu_openmp_OMP_WAIT_POLICY_active 0.10434500000000001 ms\n",
      "\n",
      "cpu_openmp_2_OMP_threads_OMP_WAIT_POLICY_passive 0.39603 ms\n",
      "\n",
      "ngraph_2_OMP_threads_OMP_WAIT_POLICY_active error\n",
      "\n",
      "ngraph_2_OMP_threads_OMP_WAIT_POLICY_passive error\n",
      "\n",
      "ngraph error\n",
      "\n",
      "ngraph_OMP_WAIT_POLICY_active error\n",
      "\n",
      "ngraph_OMP_WAIT_POLICY_passive error\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = pipeline.perf_tuning(model=model)\n",
    "#result = pipeline.perf_tuning()   # is ok, too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mklml_2_OMP_threads_OMP_WAIT_POLICY_passive 0.055078999999999996 ms\n",
      "\n",
      "mklml_2_OMP_threads_OMP_WAIT_POLICY_active 0.05899 ms\n",
      "\n",
      "mklml_OMP_WAIT_POLICY_active 0.070385 ms\n",
      "\n",
      "dnnl_OMP_WAIT_POLICY_active 0.07097 ms\n",
      "\n",
      "dnnl 0.086905 ms\n",
      "\n",
      "dnnl_2_OMP_threads_OMP_WAIT_POLICY_active 0.18291 ms\n",
      "\n",
      "nuphar 0.07813999999999999 ms\n",
      "\n",
      "nuphar_2_OMP_threads_OMP_WAIT_POLICY_active 0.07799 ms\n",
      "\n",
      "nuphar_OMP_WAIT_POLICY_passive 0.092655 ms\n",
      "\n",
      "cpu 0.079119 ms\n",
      "\n",
      "cpu_openmp_2_OMP_threads_OMP_WAIT_POLICY_active 0.092198 ms\n",
      "\n",
      "cpu_openmp_OMP_WAIT_POLICY_active 0.10434500000000001 ms\n",
      "\n",
      "cpu_openmp_2_OMP_threads_OMP_WAIT_POLICY_passive 0.39603 ms\n",
      "\n",
      "ngraph_2_OMP_threads_OMP_WAIT_POLICY_active error\n",
      "\n",
      "ngraph_2_OMP_threads_OMP_WAIT_POLICY_passive error\n",
      "\n",
      "ngraph error\n",
      "\n",
      "ngraph_OMP_WAIT_POLICY_active error\n",
      "\n",
      "ngraph_OMP_WAIT_POLICY_passive error\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline.print_performance(result)\n",
    "#pipeline.print_result() # is ok, too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Result Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performance test, there would be a directory for results. \n",
    "\n",
    "This libray use Pandas.read_json to visualize JSON file. (orient is changeable.)\n",
    "\n",
    "\"latency.json\" contains the raw data of results ordered by the average time. \n",
    "\n",
    "Use .latency to obtain the original latency JSON; Use .profiling to obtain original top 5 profiling JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ziyl.NORTHAMERICA\\OLive\\notebook/test_models\\mnist/result\n"
     ]
    }
   ],
   "source": [
    "r = pipeline.get_result(result)\n",
    "#r.latency\n",
    "#r.profiling\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print latency.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide parameters for top 5 performace. Use the parameter \"top\" to visualize more results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>avg</th>\n",
       "      <th>p90</th>\n",
       "      <th>p95</th>\n",
       "      <th>cpu_usage</th>\n",
       "      <th>gpu_usage</th>\n",
       "      <th>memory_util</th>\n",
       "      <th>code_snippet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>mklml_2_OMP_threads_OMP_WAIT_POLICY_passive</td>\n",
       "      <td>0.055079</td>\n",
       "      <td>0.0748</td>\n",
       "      <td>0.0802</td>\n",
       "      <td>0.96812</td>\n",
       "      <td>0</td>\n",
       "      <td>0.15598</td>\n",
       "      <td>OrderedDict([('execution_provider', 'mklml'), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>dnnl_OMP_WAIT_POLICY_active</td>\n",
       "      <td>0.070970</td>\n",
       "      <td>0.0934</td>\n",
       "      <td>0.1266</td>\n",
       "      <td>0.96048</td>\n",
       "      <td>0</td>\n",
       "      <td>0.15672</td>\n",
       "      <td>OrderedDict([('execution_provider', 'dnnl'), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>nuphar</td>\n",
       "      <td>0.078140</td>\n",
       "      <td>0.1143</td>\n",
       "      <td>0.1291</td>\n",
       "      <td>0.96938</td>\n",
       "      <td>0</td>\n",
       "      <td>0.15771</td>\n",
       "      <td>OrderedDict([('execution_provider', 'nuphar'),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>cpu</td>\n",
       "      <td>0.079119</td>\n",
       "      <td>0.0991</td>\n",
       "      <td>0.1245</td>\n",
       "      <td>0.94437</td>\n",
       "      <td>0</td>\n",
       "      <td>0.15657</td>\n",
       "      <td>OrderedDict([('execution_provider', 'cpu'), ('...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>cpu_openmp_2_OMP_threads_OMP_WAIT_POLICY_active</td>\n",
       "      <td>0.092198</td>\n",
       "      <td>0.1232</td>\n",
       "      <td>0.1347</td>\n",
       "      <td>0.97112</td>\n",
       "      <td>0</td>\n",
       "      <td>0.15656</td>\n",
       "      <td>OrderedDict([('execution_provider', 'cpu_openm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              name       avg     p90     p95  \\\n",
       "0      mklml_2_OMP_threads_OMP_WAIT_POLICY_passive  0.055079  0.0748  0.0802   \n",
       "1                      dnnl_OMP_WAIT_POLICY_active  0.070970  0.0934  0.1266   \n",
       "2                                           nuphar  0.078140  0.1143  0.1291   \n",
       "3                                              cpu  0.079119  0.0991  0.1245   \n",
       "4  cpu_openmp_2_OMP_threads_OMP_WAIT_POLICY_active  0.092198  0.1232  0.1347   \n",
       "\n",
       "   cpu_usage  gpu_usage  memory_util  \\\n",
       "0    0.96812          0      0.15598   \n",
       "1    0.96048          0      0.15672   \n",
       "2    0.96938          0      0.15771   \n",
       "3    0.94437          0      0.15657   \n",
       "4    0.97112          0      0.15656   \n",
       "\n",
       "                                        code_snippet  \n",
       "0  OrderedDict([('execution_provider', 'mklml'), ...  \n",
       "1  OrderedDict([('execution_provider', 'dnnl'), (...  \n",
       "2  OrderedDict([('execution_provider', 'nuphar'),...  \n",
       "3  OrderedDict([('execution_provider', 'cpu'), ('...  \n",
       "4  OrderedDict([('execution_provider', 'cpu_openm...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.prints()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print profiling.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only provide profiling JSON for top 5 performace by giving certain index of the result. The file name is profile_[name].json\n",
    "    \n",
    "    (1) index: integer\n",
    "        Required. The index for top 5 profiling files.\n",
    "    (2) top: integer\n",
    "        The number for top Ops.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>pid</th>\n",
       "      <th>tid</th>\n",
       "      <th>dur</th>\n",
       "      <th>ts</th>\n",
       "      <th>ph</th>\n",
       "      <th>name</th>\n",
       "      <th>args</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Node</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>960</td>\n",
       "      <td>14548</td>\n",
       "      <td>X</td>\n",
       "      <td>ReLU32_Output_0_nchwc_6</td>\n",
       "      <td>{'provider': 'CPUExecutionProvider', 'op_name'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Node</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>491</td>\n",
       "      <td>15563</td>\n",
       "      <td>X</td>\n",
       "      <td>ReLU114_Output_0_nchwc_11</td>\n",
       "      <td>{'provider': 'CPUExecutionProvider', 'op_name'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Node</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>440</td>\n",
       "      <td>16177</td>\n",
       "      <td>X</td>\n",
       "      <td>ReLU32_Output_0_nchwc_6</td>\n",
       "      <td>{'provider': 'CPUExecutionProvider', 'op_name'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Node</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>416</td>\n",
       "      <td>17889</td>\n",
       "      <td>X</td>\n",
       "      <td>ReLU32_Output_0_nchwc_6</td>\n",
       "      <td>{'provider': 'CPUExecutionProvider', 'op_name'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Node</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>414</td>\n",
       "      <td>18464</td>\n",
       "      <td>X</td>\n",
       "      <td>ReLU32_Output_0_nchwc_6</td>\n",
       "      <td>{'provider': 'CPUExecutionProvider', 'op_name'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat  pid  tid  dur     ts ph                       name  \\\n",
       "0  Node   93   93  960  14548  X    ReLU32_Output_0_nchwc_6   \n",
       "1  Node   93   93  491  15563  X  ReLU114_Output_0_nchwc_11   \n",
       "2  Node   93   93  440  16177  X    ReLU32_Output_0_nchwc_6   \n",
       "3  Node   93   93  416  17889  X    ReLU32_Output_0_nchwc_6   \n",
       "4  Node   93   93  414  18464  X    ReLU32_Output_0_nchwc_6   \n",
       "\n",
       "                                                args  \n",
       "0  {'provider': 'CPUExecutionProvider', 'op_name'...  \n",
       "1  {'provider': 'CPUExecutionProvider', 'op_name'...  \n",
       "2  {'provider': 'CPUExecutionProvider', 'op_name'...  \n",
       "3  {'provider': 'CPUExecutionProvider', 'op_name'...  \n",
       "4  {'provider': 'CPUExecutionProvider', 'op_name'...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.print_profiling(index=4, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get code snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import onnxruntime as ort\n",
      "so = rt.SessionOptions()\n",
      "so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
      "so.execution_mode = rt.ExecutionMode.ORT_SEQUENTIAL\n",
      "\n",
      "\n",
      "session = rt.Session(\"/mnt/model/test/model.onnx\", so, providers=[\"CPUExecutionProvider\"])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(r.get_code(ep='cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# netron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'test_models/mnist/test/model.onnx' at http://localhost:8080\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"1000\"\n",
       "            src=\"http://localhost:8080\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x241aff1afd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only workable for notebook in the local server \n",
    "import netron\n",
    "netron.start(\"test_models/mnist/test/model.onnx\", browse=False) # 'model.onnx'\n",
    "from IPython.display import IFrame\n",
    "IFrame('http://localhost:8080', width=\"100%\", height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopping http://localhost:8080\n"
     ]
    }
   ],
   "source": [
    "netron.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
