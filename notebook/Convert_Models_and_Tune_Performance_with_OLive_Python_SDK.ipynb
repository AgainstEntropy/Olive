{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Models and Tune Performance with OLive Python SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demos how to use OLive Python SDK to convert a model from other model framework to ONNX, and then tune performance for the converted ONNX model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Make sure you have [Docker](https://www.docker.com/get-started) installed and running. \n",
    "\n",
    "2) Install python dependency modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (3.2)\n",
      "Requirement already satisfied: netron in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: docker in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.25.1)\n",
      "Requirement already satisfied: onnx in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: requests!=2.18.0,>=2.14.2 in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from docker) (2.21.0)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from docker) (1.12.0)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from docker) (0.56.0)\n",
      "Requirement already satisfied: pypiwin32==223; sys_platform == \"win32\" and python_version >= \"3.6\" in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from docker) (223)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas) (1.17.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from onnx) (3.7.4.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 19.0.3, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: protobuf in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from onnx) (3.7.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests!=2.18.0,>=2.14.2->docker) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests!=2.18.0,>=2.14.2->docker) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests!=2.18.0,>=2.14.2->docker) (2019.3.9)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests!=2.18.0,>=2.14.2->docker) (1.24.1)\n",
      "Requirement already satisfied: pywin32>=223 in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pypiwin32==223; sys_platform == \"win32\" and python_version >= \"3.6\"->docker) (224)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from protobuf->onnx) (40.8.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install wget netron docker pandas onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Download OLive Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [..............................................................................] 19457 / 19457Downloaded onnxpipeline.py\n",
      "100% [................................................................................] 2421 / 2421Downloaded convert_test_data.py\n",
      "100% [..................................................................................] 709 / 709Downloaded config.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wget\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/microsoft/OLive/master/utils/\"\n",
    "sdk_files = [\"onnxpipeline.py\", \"convert_test_data.py\", \"config.py\"]\n",
    "sdk_dir = \"./python_sdk\"\n",
    "if not os.path.exists(sdk_dir):\n",
    "    os.makedirs(sdk_dir)\n",
    "\n",
    "for filename in sdk_files:\n",
    "    target_file = os.path.join(sdk_dir, filename)\n",
    "    if os.path.exists(target_file):\n",
    "        os.remove(target_file)\n",
    "    wget.download(url + filename, target_file)\n",
    "    print(\"Downloaded\", filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Pull the latest `mcr.microsoft.com/onnxruntime/onnx-converter` and `mcr.microsoft.com/onnxruntime/perf-tuning` docker images from MCR. This should take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\n",
      "latest: Pulling from onnxruntime/onnx-converter\n",
      "Digest: sha256:f279a9a704c14a1e32c0f798dfd0d2a483c7e56c08626cc47af8250987726ab2\n",
      "Status: Image is up to date for mcr.microsoft.com/onnxruntime/onnx-converter:latest\n",
      "mcr.microsoft.com/onnxruntime/onnx-converter:latest\n",
      "Using default tag: latest\n",
      "latest: Pulling from onnxruntime/perf-tuning\n",
      "Digest: sha256:8003f5ecd2e11c2fdad31610294982404954570c4ced0caed7b9b6f268e9388f\n",
      "Status: Image is up to date for mcr.microsoft.com/onnxruntime/perf-tuning:latest\n",
      "mcr.microsoft.com/onnxruntime/perf-tuning:latest\n"
     ]
    }
   ],
   "source": [
    "# pull onnx-converter and perf-tuning docker images from mcr\n",
    "!docker pull mcr.microsoft.com/onnxruntime/onnx-converter\n",
    "!docker pull mcr.microsoft.com/onnxruntime/perf-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you're ready to use OLive to convert model and tune performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convert Model To ONNX\n",
    "\n",
    "Convert the model of your choice to ONNX. The Python scripts will run OLive `onnx-converter` Docker image in the backend. It first converts the model, then tries to generate input test data if none provided, and finally runs the converted model alongside the original model for correctness check. For more information on the `onnx-converter` Docker image, please see [onnx-converter README.md](https://github.com/microsoft/OLive/blob/master/docker-images/onnx-converter/README.md)\n",
    "\n",
    "### Prepare model and test data files\n",
    "\n",
    "Put your input model file(s) and test data(optional) in one folder in the following structure:\n",
    "\n",
    "    - your_folder\n",
    "       - test_data_set_0\n",
    "           - input_0.pb\n",
    "           - ...\n",
    "           - output_0.pb\n",
    "           - ...\n",
    "       - model_file(s)\n",
    "\n",
    "\n",
    "Supported model frameworks are - cntk, coreml, keras, scikit-learn, tensorflow and pytorch. \n",
    "\n",
    "As an example, we use [MNIST model from ONNX model zoo](https://github.com/onnx/models/tree/master/vision/classification/mnist) for the following tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Download and store the model to the desire directory. Modify these lines to point to your local model directory \n",
    "model_url = \"https://onnxzoo.blob.core.windows.net/models/opset_8/mnist/mnist.tar.gz\"\n",
    "model_dir = \"test_models\"\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "import tarfile\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "file_tmp = urlretrieve(model_url, filename=None)[0]\n",
    "\n",
    "tar = tarfile.open(file_tmp)\n",
    "tar.extractall(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the test data, we strongly recommend you provide your own test input and output files. However if no input files are provided, OLive will randomly generate dummy inputs if possible. \n",
    "\n",
    "ONNX .pb files are expected. If you're more familiar with pickle files, dump your input data to a single pickle file in the following dict format - \n",
    "\n",
    "    {\n",
    "        \"input_name_0\": input_data_0,\n",
    "        \"input_name_1\": input_data_1, \n",
    "        ...\n",
    "    }\n",
    "or if dumping output data - \n",
    "    {\n",
    "        \"output_name_0\": output_data_0,\n",
    "        \"output_name_1\": output_data_1, \n",
    "        ...\n",
    "    }\n",
    "\n",
    "Then use the `convert_data_to_pb.py` script to convert your pickle file to pb files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./python/convert_data_to_pb.py <your_input_pickle_file> --output_folder <output_folder> --is_input=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate onnxpipeline\n",
    "\n",
    "After the model files are stored in a directory, we can pass the directory into onnxpipeline to start the model conversion. The parameters for onnxpipeline are listed below. \n",
    "\n",
    "    (1) local_directory: string\n",
    "        Required. The path of local directory where would be mounted to the docker. All operations will be executed from this path.\n",
    "\n",
    "    (2) mount_path: string\n",
    "        Optional. The path where the local_directory will be mounted in the docker. Default is \"/mnt/model\".\n",
    "\n",
    "    (3) print_logs: boolean\n",
    "        Optional. Whether print the logs from the docker. Default is True.\n",
    "\n",
    "    (4) convert_directory: string\n",
    "        Optional. The directory path for converting model. Default is test/.    \n",
    "\n",
    "    (5) convert_name: string\n",
    "        Optional. The model name for converting model. Default is model.onnx.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------config----------------\n",
      "           Container information: <docker.client.DockerClient object at 0x000001E4EB9331D0>\n",
      " Local directory path for volume: C:\\Users\\ziyl.NORTHAMERICA\\OLive\\notebook/test_models\\mnist\n",
      "Volume directory path in dockers: /mnt/model\n",
      "                     Result path: result\n",
      "        Converted directory path: test\n",
      "        Converted model filename: model.onnx\n",
      "            Converted model path: test/model.onnx\n",
      "        Print logs in the docker: True\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('./python_sdk')\n",
    "import onnxpipeline\n",
    "\n",
    "# Initiate ONNX pipeline with your model path\n",
    "pipeline = onnxpipeline.Pipeline(os.path.join(model_dir, 'mnist'))\n",
    "\n",
    "# Check the configs\n",
    "pipeline.config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Model From Various Frameworks to ONNX Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLive is capable of converting models from most major frameworks to ONNX. Different frameworks may require different inputs. Check the commented code below to see what parameters are required for each supported model framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:root:TensorFlow version 1.15.0 detected. Last version known to be fully compatible is 1.14.0 .\n",
      "\n",
      "\n",
      "\n",
      "-------------\n",
      "\n",
      "Model Conversion\n",
      "\n",
      "\n",
      "\n",
      "Input model is already ONNX model. Skipping conversion.\n",
      "\n",
      "\n",
      "\n",
      "-------------\n",
      "\n",
      "MODEL INPUT GENERATION(if needed)\n",
      "\n",
      "\n",
      "\n",
      "/mnt/model/test/model.onnx inputs: \n",
      "\n",
      "input name: Input3, shape: [1, 1, 28, 28], type: tensor(float)\n",
      "\n",
      "Randomized input .pb file generated at  /mnt/model/test/test_data_set_0\n",
      "\n",
      "\n",
      "\n",
      "-------------\n",
      "\n",
      "MODEL CORRECTNESS VERIFICATION\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Check the ONNX model for validity \n",
      "\n",
      "The ONNX model is valid.\n",
      "\n",
      "\n",
      "\n",
      "The original model is already onnx. Skipping correctness test. \n",
      "\n",
      "\n",
      "\n",
      "-------------\n",
      "\n",
      "MODEL CONVERSION SUMMARY (.json file generated at /mnt/model/test/output.json )\n",
      "\n",
      "\n",
      "\n",
      "{'conversion_status': 'SUCCESS',\n",
      "\n",
      " 'correctness_verified': 'SKIPPED',\n",
      "\n",
      " 'error_message': '',\n",
      "\n",
      " 'input_folder': '/mnt/model/test/test_data_set_0',\n",
      "\n",
      " 'output_onnx_path': '/mnt/model/test/model.onnx'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = pipeline.convert_model(model='model.onnx', model_type='onnx')\n",
    "\n",
    "# test tensorflow savedmodel (no need to specify \"model=\" as it takes a directory as input)\n",
    "# model = pipeline.convert_model(model_type='tensorflow')\n",
    "\n",
    "# test tensorflow savedgraph and checkpoints\n",
    "# model = pipeline.convert_model(model_type='tensorflow', model='saved_model.pb/meta', \n",
    "#                                model_input_names=\"input_name_1,input_name_2...\", \n",
    "#                                model_output_names=\"output_name_1,output_name_2...\")\n",
    "\n",
    "# test pytorch\n",
    "# model = pipeline.convert_model(model_type='pytorch', model='saved_model.pb', model_input_shapes='(1,3,224,224)')\n",
    "\n",
    "# test cntk\n",
    "# model = pipeline.convert_model(model_type='cntk', model='ResNet50_ImageNet_Caffe.model')\n",
    "\n",
    "# test keras\n",
    "# model = pipeline.convert_model(model_type='keras', model='keras_Average_ImageNet.keras')\n",
    "\n",
    "# test sklearn\n",
    "# model = pipeline.convert_model(model_type='scikit-learn', model='sklearn_svc.joblib', initial_types=(\"float_input\", \"FloatTensorType([1,4])\"))\n",
    "\n",
    "# test caffe\n",
    "# model = pipeline.convert_model(model_type='caffe', model='bvlc_alexnet.caffemodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Performance Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have an ONNX model and its test data, you can run performance tuning using OLive Python SDK `pipeline.perf_tuning` with your desired options. A complete list of available options are listed below. \n",
    "\n",
    "    (1) model: string\n",
    "\n",
    "        Required. The path of the model that wants to be performed.\n",
    "\n",
    "    (2) result: string\n",
    "\n",
    "        Optional. The path of the result.\n",
    "\n",
    "    (3) config: string (choices=[\"Debug\", \"MinSizeRel\", \"Release\", \"RelWithDebInfo\"])\n",
    "\n",
    "        Optional. Configuration to run. Default is \"RelWithDebInfo\".\n",
    "\n",
    "    (4) test_mode: string (choices=[\"duration\", \"times\"])\n",
    "\n",
    "        Optional. Specifies the test mode. Value could be 'duration' or 'times'. Default is \"times\".\n",
    "\n",
    "    (5) execution_provider: string (choices=[\"cpu\", \"cuda\", \"mkldnn\"])\n",
    "\n",
    "        Optional. help=\"Specifies the provider 'cpu','cuda','mkldnn'. Default is ''.\n",
    "\n",
    "    (6) repeated_times: integer\n",
    "\n",
    "        Optional. Specifies the repeated times if running in 'times' test mode. Default:20.\n",
    "\n",
    "    (7) duration_times: integer\n",
    "\n",
    "        Optional. Specifies the seconds to run for 'duration' mode. Default:10.\n",
    "\n",
    "    (8) parallel: boolean\n",
    "\n",
    "        Optional. Use parallel executor, default (without -x): sequential executor.\n",
    "\n",
    "    (9) intra_op_num_threads: integer\n",
    "\n",
    "        Optional. Sets the number of threads used to parallelize the execution within nodes, A value of 0 means ORT will pick a default. Must >=0.\n",
    "\n",
    "    (10) inter_op_num_threads: integer\n",
    "\n",
    "        Optional. Sets the number of threads used to parallelize the execution of the graph (across nodes), A value of 0 means ORT will pick a default. Must >=0.\n",
    "\n",
    "    (11) top_n: integer\n",
    "\n",
    "        Optional. Show percentiles for top n runs. Default:5.\n",
    "\n",
    "    (12) runtime: boolean\n",
    "\n",
    "        Optional. Use this boolean flag to enable GPU if you have one.\n",
    "\n",
    "    (13) input_json: string\n",
    "\n",
    "        Optional. Use JSON file as input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.00315954 s\n",
      "\n",
      "Total inference time cost:0.000848683 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.0424342 ms\n",
      "\n",
      "Total inference run time:0.000865692 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.00349797 s\n",
      "\n",
      "Total inference time cost:0.00677852 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.338926 ms\n",
      "\n",
      "Total inference run time:0.00681363 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.00441003 s\n",
      "\n",
      "Total inference time cost:0.000592256 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.0296128 ms\n",
      "\n",
      "Total inference run time:0.000607534 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.00282999 s\n",
      "\n",
      "Total inference time cost:0.000880639 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.044032 ms\n",
      "\n",
      "Total inference run time:0.000896675 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.00280605 s\n",
      "\n",
      "Total inference time cost:0.0065013 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.325065 ms\n",
      "\n",
      "Total inference run time:0.00656023 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.00282685 s\n",
      "\n",
      "Total inference time cost:0.000804809 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.0402405 ms\n",
      "\n",
      "Total inference run time:0.000818134 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.00298025 s\n",
      "\n",
      "Total inference time cost:0.000765376 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.0382688 ms\n",
      "\n",
      "Total inference run time:0.000778266 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.00391896 s\n",
      "\n",
      "Total inference time cost:0.000943476 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.0471738 ms\n",
      "\n",
      "Total inference run time:0.000960808 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.00291135 s\n",
      "\n",
      "Total inference time cost:0.000756272 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.0378136 ms\n",
      "\n",
      "Total inference run time:0.000768734 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.00534246 s\n",
      "\n",
      "Total inference time cost:0.00120445 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.0602224 ms\n",
      "\n",
      "Total inference run time:0.00122817 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.00317124 s\n",
      "\n",
      "Total inference time cost:0.000952356 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.0476178 ms\n",
      "\n",
      "Total inference run time:0.000967525 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.00293291 s\n",
      "\n",
      "Total inference time cost:0.00535773 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.267887 ms\n",
      "\n",
      "Total inference run time:0.00538027 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.00318186 s\n",
      "\n",
      "Total inference time cost:0.000797442 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.0398721 ms\n",
      "\n",
      "Total inference run time:0.000812175 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.00308577 s\n",
      "\n",
      "Total inference time cost:0.000761804 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.0380902 ms\n",
      "\n",
      "Total inference run time:0.000775883 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.00287008 s\n",
      "\n",
      "Total inference time cost:0.00511669 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.255835 ms\n",
      "\n",
      "Total inference run time:0.0051376 s\n",
      "\n",
      "Setting intra_op_num_threads to 0\n",
      "\n",
      "Session creation time cost:0.00279067 s\n",
      "\n",
      "Total inference time cost:0.00124648 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.0623241 ms\n",
      "\n",
      "Total inference run time:0.00127021 s\n",
      "\n",
      "/home/ziyl/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:107 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] /home/ziyl/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:101 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] CUDA failure 35: CUDA driver version is insufficient for CUDA runtime version ; GPU=32636 ; hostname=22f0f9a582a3 ; expr=cudaSetDevice(device_id_); \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Setting intra_op_num_threads to 0\n",
      "\n",
      "/home/ziyl/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:107 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] /home/ziyl/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:101 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] CUDA failure 35: CUDA driver version is insufficient for CUDA runtime version ; GPU=32661 ; hostname=22f0f9a582a3 ; expr=cudaSetDevice(device_id_); \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Setting intra_op_num_threads to 0\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "[16:35:21] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "\n",
      "[16:35:21] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "\n",
      "[16:35:21] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=32\n",
      "\n",
      "Session creation time cost:0.0681628 s\n",
      "\n",
      "Total inference time cost:0.00106502 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.0532511 ms\n",
      "\n",
      "Total inference run time:0.00107932 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "[16:35:21] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "\n",
      "[16:35:21] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "\n",
      "[16:35:21] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=32\n",
      "\n",
      "Session creation time cost:0.0704652 s\n",
      "\n",
      "Total inference time cost:0.00114562 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.0572812 ms\n",
      "\n",
      "Total inference run time:0.00116058 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "[16:35:21] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "\n",
      "[16:35:21] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "\n",
      "[16:35:21] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=32\n",
      "\n",
      "Session creation time cost:0.0697394 s\n",
      "\n",
      "Total inference time cost:0.00110197 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.0550983 ms\n",
      "\n",
      "Total inference run time:0.00111572 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "[16:35:21] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "\n",
      "[16:35:21] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "\n",
      "[16:35:21] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=32\n",
      "\n",
      "Session creation time cost:0.067976 s\n",
      "\n",
      "Total inference time cost:0.0012389 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.0619451 ms\n",
      "\n",
      "Total inference run time:0.00125461 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "[16:35:22] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "\n",
      "[16:35:22] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "\n",
      "[16:35:22] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=32\n",
      "\n",
      "Session creation time cost:0.0688333 s\n",
      "\n",
      "Total inference time cost:0.00106167 s\n",
      "\n",
      "Total inference requests:20\n",
      "\n",
      "Average inference time cost:0.0530835 ms\n",
      "\n",
      "Total inference run time:0.00107564 s\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.00294927 s\n",
      "\n",
      "Total inference time cost:0.00769274 s\n",
      "\n",
      "Total inference requests:200\n",
      "\n",
      "Average inference time cost:0.0384637 ms\n",
      "\n",
      "Total inference run time:0.00783142 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.00296032 s\n",
      "\n",
      "Total inference time cost:0.00783238 s\n",
      "\n",
      "Total inference requests:200\n",
      "\n",
      "Average inference time cost:0.0391619 ms\n",
      "\n",
      "Total inference run time:0.0079417 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "Session creation time cost:0.00314448 s\n",
      "\n",
      "Total inference time cost:0.00724946 s\n",
      "\n",
      "Total inference requests:200\n",
      "\n",
      "Average inference time cost:0.0362473 ms\n",
      "\n",
      "Total inference run time:0.00737046 s\n",
      "\n",
      "Setting intra_op_num_threads to 0\n",
      "\n",
      "Session creation time cost:0.00352852 s\n",
      "\n",
      "Total inference time cost:0.00757413 s\n",
      "\n",
      "Total inference requests:200\n",
      "\n",
      "Average inference time cost:0.0378706 ms\n",
      "\n",
      "Total inference run time:0.00769708 s\n",
      "\n",
      "Setting intra_op_num_threads to 1\n",
      "\n",
      "[16:35:24] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "\n",
      "[16:35:24] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "\n",
      "[16:35:24] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=32\n",
      "\n",
      "Session creation time cost:0.0689706 s\n",
      "\n",
      "Total inference time cost:0.0102373 s\n",
      "\n",
      "Total inference requests:200\n",
      "\n",
      "Average inference time cost:0.0511864 ms\n",
      "\n",
      "Total inference run time:0.010342 s\n",
      "\n",
      "Cores:  2\n",
      "\n",
      "No GPU found on current device. Cuda and TensorRT performance tuning might not be available. \n",
      "\n",
      "providers  ['cpu_openmp', 'mklml', 'dnnl', 'cpu', 'tensorrt', 'ngraph', 'cuda', 'nuphar']\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "OMP_NUM_THREADS=2\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/5f42e81b-9f27-4efe-9f65-e6e6290a34a2\n",
      "\n",
      "cpu_openmp_2_OMP_threads_OMP_WAIT_POLICY_active 0.042434\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "\n",
      "OMP_NUM_THREADS=2\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/6f6bf5cd-963d-44bf-b1ce-e0906ac9db0a\n",
      "\n",
      "cpu_openmp_2_OMP_threads_OMP_WAIT_POLICY_passive 0.33892599999999995\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/a429285d-ed3d-4e47-ba80-138059be35ab\n",
      "\n",
      "cpu_openmp 0.029613\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/186b215f-bc1f-4d5f-8829-dd97e9adf261\n",
      "\n",
      "cpu_openmp_OMP_WAIT_POLICY_active 0.044032\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/e667ae8d-586b-4504-9218-a09cd1c9a4a6\n",
      "\n",
      "cpu_openmp_OMP_WAIT_POLICY_passive 0.325065\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "OMP_NUM_THREADS=2\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/b2500d3f-412c-46ff-8431-1452b1f5bc95\n",
      "\n",
      "mklml_2_OMP_threads_OMP_WAIT_POLICY_active 0.04024\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "\n",
      "OMP_NUM_THREADS=2\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/58231f02-5668-4a76-bbc9-89b0c66a031c\n",
      "\n",
      "mklml_2_OMP_threads_OMP_WAIT_POLICY_passive 0.038269000000000004\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/56ac4fa3-1d16-4bfb-af08-032bdb592717\n",
      "\n",
      "mklml 0.047174\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/9d9c9729-3d9e-4830-8c0c-b590e503e266\n",
      "\n",
      "mklml_OMP_WAIT_POLICY_active 0.037814\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/d857a610-f446-4916-a446-f99d7eeddf17\n",
      "\n",
      "mklml_OMP_WAIT_POLICY_passive 0.060222\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "OMP_NUM_THREADS=2\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/2c53641b-331c-4893-b962-74dfad6eec22\n",
      "\n",
      "dnnl_2_OMP_threads_OMP_WAIT_POLICY_active 0.047617999999999994\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "\n",
      "OMP_NUM_THREADS=2\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/106d46c6-1972-4620-9524-0415b5ae778a\n",
      "\n",
      "dnnl_2_OMP_threads_OMP_WAIT_POLICY_passive 0.26788700000000004\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/b8d91736-87be-4c13-8b25-01ec5ba7c3a2\n",
      "\n",
      "dnnl 0.039872\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/7858723d-5ab4-4a78-befe-8956dfba3b70\n",
      "\n",
      "dnnl_OMP_WAIT_POLICY_active 0.038090000000000006\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/b5f9e1a1-7066-4f9e-aa2c-1f61a4c428c1\n",
      "\n",
      "dnnl_OMP_WAIT_POLICY_passive 0.255835\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e cpu -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/b2f1d5c6-ccb4-4be4-a46b-238a5726ac74\n",
      "\n",
      "cpu 0.062324\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e tensorrt -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/5126ad9c-1e70-4e99-bc71-f1d9aa697f00\n",
      "\n",
      "tensorrt None\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "OMP_NUM_THREADS=2\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/ngraph/onnxruntime_perf_test -e ngraph -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/ab160a5c-c009-48b8-80d3-051087766e2d\n",
      "\n",
      "ngraph_2_OMP_threads_OMP_WAIT_POLICY_active None\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "\n",
      "OMP_NUM_THREADS=2\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/ngraph/onnxruntime_perf_test -e ngraph -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/345f46f3-8dbc-42e0-9476-d21196d635b8\n",
      "\n",
      "ngraph_2_OMP_threads_OMP_WAIT_POLICY_passive None\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/ngraph/onnxruntime_perf_test -e ngraph -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/f6c166f8-353f-442a-a6ea-eb1974827aeb\n",
      "\n",
      "ngraph None\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/ngraph/onnxruntime_perf_test -e ngraph -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/7f346546-f41b-40c6-a96f-1d917dec95b4\n",
      "\n",
      "ngraph_OMP_WAIT_POLICY_active None\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/ngraph/onnxruntime_perf_test -e ngraph -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/96085867-b9fd-497e-b9cf-8a7f715c2d8a\n",
      "\n",
      "ngraph_OMP_WAIT_POLICY_passive None\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e cuda -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/14f7dd39-650f-48f0-9494-910156a53312\n",
      "\n",
      "cuda None\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "OMP_NUM_THREADS=2\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -e nuphar -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/de81e8c4-87e9-44ce-a7cf-4cc3600e63a7\n",
      "\n",
      "nuphar_2_OMP_threads_OMP_WAIT_POLICY_active 0.053251\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "\n",
      "OMP_NUM_THREADS=2\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -e nuphar -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/66b70049-4731-4919-a13e-f1af3adebc6d\n",
      "\n",
      "nuphar_2_OMP_threads_OMP_WAIT_POLICY_passive 0.057281000000000006\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -e nuphar -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/1f749612-6312-4723-ad98-f1486e03554c\n",
      "\n",
      "nuphar 0.055098\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -e nuphar -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/e7b9485f-da42-4f84-9869-b0d3e80fe731\n",
      "\n",
      "nuphar_OMP_WAIT_POLICY_active 0.06194500000000001\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -e nuphar -x 1 -o 99 -m times -r 20 /mnt/model/test/model.onnx /mnt/model/result/02be560d-f2c1-4862-86f6-7d90756b7e53\n",
      "\n",
      "nuphar_OMP_WAIT_POLICY_passive 0.053083\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -x 1 -o 99 -m times -r 200 /mnt/model/test/model.onnx /mnt/model/result/d17f687d-dec2-4fa8-9e5f-0697ed183cb2\n",
      "\n",
      "cpu_openmp 0.038464000000000005\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -x 1 -o 99 -m times -r 200 /mnt/model/test/model.onnx /mnt/model/result/fb1732ee-e724-47f9-87be-ad9f735d72a3\n",
      "\n",
      "mklml_OMP_WAIT_POLICY_active 0.039162\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -x 1 -o 99 -m times -r 200 /mnt/model/test/model.onnx /mnt/model/result/dbc97819-d7cb-4d5f-b01b-912f3656f6bb\n",
      "\n",
      "dnnl_OMP_WAIT_POLICY_active 0.036247\n",
      "\n",
      "\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e cpu -o 99 -m times -r 200 /mnt/model/test/model.onnx /mnt/model/result/0598a7be-52fd-4ec2-b32e-a25f5cee3856\n",
      "\n",
      "cpu 0.037870999999999995\n",
      "\n",
      "\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -e nuphar -x 1 -o 99 -m times -r 200 /mnt/model/test/model.onnx /mnt/model/result/53444f89-9d70-4445-aff7-9b517abb5669\n",
      "\n",
      "nuphar_OMP_WAIT_POLICY_passive 0.051186\n",
      "\n",
      "\n",
      "\n",
      "Results:\n",
      "\n",
      "dnnl_OMP_WAIT_POLICY_active 0.036247 ms\n",
      "\n",
      "dnnl 0.039872 ms\n",
      "\n",
      "dnnl_2_OMP_threads_OMP_WAIT_POLICY_active 0.047617999999999994 ms\n",
      "\n",
      "cpu 0.037870999999999995 ms\n",
      "\n",
      "cpu_openmp 0.038464000000000005 ms\n",
      "\n",
      "cpu_openmp_2_OMP_threads_OMP_WAIT_POLICY_active 0.042434 ms\n",
      "\n",
      "cpu_openmp_OMP_WAIT_POLICY_active 0.044032 ms\n",
      "\n",
      "mklml_OMP_WAIT_POLICY_active 0.039162 ms\n",
      "\n",
      "mklml_2_OMP_threads_OMP_WAIT_POLICY_passive 0.038269000000000004 ms\n",
      "\n",
      "mklml_2_OMP_threads_OMP_WAIT_POLICY_active 0.04024 ms\n",
      "\n",
      "nuphar_OMP_WAIT_POLICY_passive 0.051186 ms\n",
      "\n",
      "nuphar_2_OMP_threads_OMP_WAIT_POLICY_active 0.053251 ms\n",
      "\n",
      "nuphar 0.055098 ms\n",
      "\n",
      "ngraph_2_OMP_threads_OMP_WAIT_POLICY_active error\n",
      "\n",
      "ngraph_2_OMP_threads_OMP_WAIT_POLICY_passive error\n",
      "\n",
      "ngraph error\n",
      "\n",
      "ngraph_OMP_WAIT_POLICY_active error\n",
      "\n",
      "ngraph_OMP_WAIT_POLICY_passive error\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = pipeline.perf_tuning(model=model)\n",
    "#result = pipeline.perf_tuning()   # is ok, too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dnnl_OMP_WAIT_POLICY_active 0.036247 ms\n",
      "\n",
      "dnnl 0.039872 ms\n",
      "\n",
      "dnnl_2_OMP_threads_OMP_WAIT_POLICY_active 0.047617999999999994 ms\n",
      "\n",
      "cpu 0.037870999999999995 ms\n",
      "\n",
      "cpu_openmp 0.038464000000000005 ms\n",
      "\n",
      "cpu_openmp_2_OMP_threads_OMP_WAIT_POLICY_active 0.042434 ms\n",
      "\n",
      "cpu_openmp_OMP_WAIT_POLICY_active 0.044032 ms\n",
      "\n",
      "mklml_OMP_WAIT_POLICY_active 0.039162 ms\n",
      "\n",
      "mklml_2_OMP_threads_OMP_WAIT_POLICY_passive 0.038269000000000004 ms\n",
      "\n",
      "mklml_2_OMP_threads_OMP_WAIT_POLICY_active 0.04024 ms\n",
      "\n",
      "nuphar_OMP_WAIT_POLICY_passive 0.051186 ms\n",
      "\n",
      "nuphar_2_OMP_threads_OMP_WAIT_POLICY_active 0.053251 ms\n",
      "\n",
      "nuphar 0.055098 ms\n",
      "\n",
      "ngraph_2_OMP_threads_OMP_WAIT_POLICY_active error\n",
      "\n",
      "ngraph_2_OMP_threads_OMP_WAIT_POLICY_passive error\n",
      "\n",
      "ngraph error\n",
      "\n",
      "ngraph_OMP_WAIT_POLICY_active error\n",
      "\n",
      "ngraph_OMP_WAIT_POLICY_passive error\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline.print_performance(result)\n",
    "#pipeline.print_result() # is ok, too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Result Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performance test, there would be a directory for results. \n",
    "\n",
    "This libray use Pandas.read_json to visualize JSON file. (orient is changeable.)\n",
    "\n",
    "\"latency.json\" contains the raw data of results ordered by the average time. \n",
    "\n",
    "Use .latency to obtain the original latency JSON; Use .profiling to obtain original top 5 profiling JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ziyl.NORTHAMERICA\\OLive\\notebook/test_models\\mnist/result\n"
     ]
    }
   ],
   "source": [
    "r = pipeline.get_result(result)\n",
    "#r.latency\n",
    "#r.profiling\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print latency.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide parameters for top 5 performace. Use the parameter \"top\" to visualize more results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>avg</th>\n",
       "      <th>p90</th>\n",
       "      <th>p95</th>\n",
       "      <th>cpu_usage</th>\n",
       "      <th>gpu_usage</th>\n",
       "      <th>memory_util</th>\n",
       "      <th>code_snippet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>dnnl_OMP_WAIT_POLICY_active</td>\n",
       "      <td>0.036247</td>\n",
       "      <td>0.04734</td>\n",
       "      <td>0.05655</td>\n",
       "      <td>0.92775</td>\n",
       "      <td>0</td>\n",
       "      <td>0.22406</td>\n",
       "      <td>OrderedDict([('execution_provider', 'dnnl'), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>cpu</td>\n",
       "      <td>0.037871</td>\n",
       "      <td>0.05427</td>\n",
       "      <td>0.06619</td>\n",
       "      <td>0.93150</td>\n",
       "      <td>0</td>\n",
       "      <td>0.22399</td>\n",
       "      <td>OrderedDict([('execution_provider', 'cpu'), ('...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>cpu_openmp</td>\n",
       "      <td>0.038464</td>\n",
       "      <td>0.04973</td>\n",
       "      <td>0.05948</td>\n",
       "      <td>0.93689</td>\n",
       "      <td>0</td>\n",
       "      <td>0.22284</td>\n",
       "      <td>OrderedDict([('execution_provider', 'cpu_openm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>mklml_OMP_WAIT_POLICY_active</td>\n",
       "      <td>0.039162</td>\n",
       "      <td>0.04550</td>\n",
       "      <td>0.05308</td>\n",
       "      <td>0.87462</td>\n",
       "      <td>0</td>\n",
       "      <td>0.22381</td>\n",
       "      <td>OrderedDict([('execution_provider', 'mklml'), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>nuphar_OMP_WAIT_POLICY_passive</td>\n",
       "      <td>0.051186</td>\n",
       "      <td>0.06522</td>\n",
       "      <td>0.08461</td>\n",
       "      <td>0.96937</td>\n",
       "      <td>0</td>\n",
       "      <td>0.22517</td>\n",
       "      <td>OrderedDict([('execution_provider', 'nuphar'),...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             name       avg      p90      p95  cpu_usage  \\\n",
       "0     dnnl_OMP_WAIT_POLICY_active  0.036247  0.04734  0.05655    0.92775   \n",
       "1                             cpu  0.037871  0.05427  0.06619    0.93150   \n",
       "2                      cpu_openmp  0.038464  0.04973  0.05948    0.93689   \n",
       "3    mklml_OMP_WAIT_POLICY_active  0.039162  0.04550  0.05308    0.87462   \n",
       "4  nuphar_OMP_WAIT_POLICY_passive  0.051186  0.06522  0.08461    0.96937   \n",
       "\n",
       "   gpu_usage  memory_util                                       code_snippet  \n",
       "0          0      0.22406  OrderedDict([('execution_provider', 'dnnl'), (...  \n",
       "1          0      0.22399  OrderedDict([('execution_provider', 'cpu'), ('...  \n",
       "2          0      0.22284  OrderedDict([('execution_provider', 'cpu_openm...  \n",
       "3          0      0.22381  OrderedDict([('execution_provider', 'mklml'), ...  \n",
       "4          0      0.22517  OrderedDict([('execution_provider', 'nuphar'),...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.prints()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print profiling.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only provide profiling JSON for top 5 performace by giving certain index of the result. The file name is profile_[name].json\n",
    "    \n",
    "    (1) index: integer\n",
    "        Required. The index for top 5 profiling files.\n",
    "    (2) top: integer\n",
    "        The number for top Ops.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>pid</th>\n",
       "      <th>tid</th>\n",
       "      <th>dur</th>\n",
       "      <th>ts</th>\n",
       "      <th>ph</th>\n",
       "      <th>name</th>\n",
       "      <th>args</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Node</td>\n",
       "      <td>12569</td>\n",
       "      <td>12569</td>\n",
       "      <td>1404</td>\n",
       "      <td>34769</td>\n",
       "      <td>X</td>\n",
       "      <td></td>\n",
       "      <td>{'provider': 'CPUExecutionProvider', 'op_name'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Node</td>\n",
       "      <td>12569</td>\n",
       "      <td>12569</td>\n",
       "      <td>272</td>\n",
       "      <td>33080</td>\n",
       "      <td>X</td>\n",
       "      <td>distill_conv1_1_nchwc_9</td>\n",
       "      <td>{'provider': 'CPUExecutionProvider', 'op_name'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Node</td>\n",
       "      <td>12569</td>\n",
       "      <td>12569</td>\n",
       "      <td>230</td>\n",
       "      <td>40891</td>\n",
       "      <td>X</td>\n",
       "      <td>distill_conv2/bn_1_nchwc_14</td>\n",
       "      <td>{'provider': 'CPUExecutionProvider', 'op_name'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Node</td>\n",
       "      <td>12569</td>\n",
       "      <td>12569</td>\n",
       "      <td>215</td>\n",
       "      <td>56671</td>\n",
       "      <td>X</td>\n",
       "      <td>distill_conv2/bn_1_nchwc_14</td>\n",
       "      <td>{'provider': 'CPUExecutionProvider', 'op_name'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Node</td>\n",
       "      <td>12569</td>\n",
       "      <td>12569</td>\n",
       "      <td>205</td>\n",
       "      <td>49507</td>\n",
       "      <td>X</td>\n",
       "      <td>distill_conv2/bn_1_nchwc_14</td>\n",
       "      <td>{'provider': 'CPUExecutionProvider', 'op_name'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat    pid    tid   dur     ts ph                         name  \\\n",
       "0  Node  12569  12569  1404  34769  X                                \n",
       "1  Node  12569  12569   272  33080  X      distill_conv1_1_nchwc_9   \n",
       "2  Node  12569  12569   230  40891  X  distill_conv2/bn_1_nchwc_14   \n",
       "3  Node  12569  12569   215  56671  X  distill_conv2/bn_1_nchwc_14   \n",
       "4  Node  12569  12569   205  49507  X  distill_conv2/bn_1_nchwc_14   \n",
       "\n",
       "                                                args  \n",
       "0  {'provider': 'CPUExecutionProvider', 'op_name'...  \n",
       "1  {'provider': 'CPUExecutionProvider', 'op_name'...  \n",
       "2  {'provider': 'CPUExecutionProvider', 'op_name'...  \n",
       "3  {'provider': 'CPUExecutionProvider', 'op_name'...  \n",
       "4  {'provider': 'CPUExecutionProvider', 'op_name'...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.print_profiling(index=4, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get code snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import onnxruntime as ort\n",
      "so = rt.SessionOptions()\n",
      "so.set_graph_optimization_level(99)\n",
      "so.enable_sequential_execution = False\n",
      "so.session_thread_pool_size(0)\n",
      "session = rt.Session(\"/mnt/model/test/model.onnx\", so)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(r.get_code(ep='cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# netron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'model/test/model.onnx' at http://localhost:8080\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"1000\"\n",
       "            src=\"http://localhost:8080\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1e5d8f7d978>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only workable for notebook in the local server \n",
    "import netron\n",
    "netron.start(\"model/test/model.onnx\", browse=False) # 'model.onnx'\n",
    "from IPython.display import IFrame\n",
    "IFrame('http://localhost:8080', width=\"100%\", height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopping http://localhost:8080\n"
     ]
    }
   ],
   "source": [
    "netron.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
