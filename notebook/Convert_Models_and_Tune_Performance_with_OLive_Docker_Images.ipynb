{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License.\n",
    "\n",
    "# Convert Models and Tune Performance with OLive Docker Images\n",
    "\n",
    "This notebook demos how to use OLive Docker images `onnx-converter` and `perf-tuning` to convert a model from another model framework to ONNX, and then tune performance for the converted ONNX model.\n",
    "\n",
    "# 0. Prerequisites\n",
    "\n",
    "1) Make sure you have [Docker](https://www.docker.com/get-started) installed and running. \n",
    "\n",
    "2) Familiar with basic Docker terminologies and commands such as volumes.\n",
    "\n",
    "3) Pull the latest `onnx-converter` and `perf-tuning` docker images from MCR. This should take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\n",
      "latest: Pulling from onnxruntime/onnx-converter\n",
      "Digest: sha256:16b93a030157c399670e409818f321b6779bb0f34ee7cbc1aa0f536fafa38b09\n",
      "Status: Image is up to date for mcr.microsoft.com/onnxruntime/onnx-converter:latest\n",
      "mcr.microsoft.com/onnxruntime/onnx-converter:latest\n",
      "Using default tag: latest\n",
      "latest: Pulling from onnxruntime/perf-tuning\n",
      "Digest: sha256:25821ab15af18c69e629cc8ad0f9d38752394972d32297a4c374527d9a05703a\n",
      "Status: Image is up to date for mcr.microsoft.com/onnxruntime/perf-tuning:latest\n",
      "mcr.microsoft.com/onnxruntime/perf-tuning:latest\n"
     ]
    }
   ],
   "source": [
    "# pull onnx-converter and perf-tuning docker images from mcr\n",
    "!docker pull mcr.microsoft.com/onnxruntime/onnx-converter\n",
    "!docker pull mcr.microsoft.com/onnxruntime/perf-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Install dependencies to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\ziyl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (3.2)\n",
      "Requirement already satisfied: netron in c:\\users\\ziyl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (4.6.8)\n",
      "Requirement already satisfied: onnx in c:\\users\\ziyl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.8.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\ziyl\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\ziyl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from onnx) (1.19.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in c:\\users\\ziyl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from onnx) (3.7.4.3)\n",
      "Requirement already satisfied: six in c:\\users\\ziyl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from onnx) (1.15.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\ziyl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from onnx) (3.11.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ziyl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from protobuf->onnx) (47.1.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install wget netron onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convert Model To ONNX\n",
    "\n",
    "The first step in OLive is to convert a model of your choice to ONNX using `onnx-converter` Docker image. The Docker image first converts the model to ONNX, then tries to generate input test data if none provided, and finally runs the converted model alongside the original model for correctness check. \n",
    "\n",
    "### Prepare model and test data files\n",
    "\n",
    "First you'll need to prepare your model and test data files. Supported model frameworks are - cntk, coreml, keras, scikit-learn, tensorflow and pytorch. \n",
    "\n",
    "For `onnx-converter`, test data are used for a quick verification of correctness for the converted model. We strongly recommend you provide your own test input and output files. However if no input files are provided, OLive will randomly generate dummy inputs for you if possible. Only one test data set is needed, but feel free to put in more data sets as they will be available for the next `perf-tuning` step to use. \n",
    "\n",
    "You can put your test data in one of - \n",
    "\n",
    "  1) Your input folder with your model from another framework.\n",
    "  \n",
    "  2) Your output folder created in advance to hold your converted ONNX model.\n",
    "  \n",
    "  3) Any other location. Need to specify the path with `--test_data_path` parameter to `onnx-converter`.\n",
    "  \n",
    "The best practice to put your input model file(s) and test data(optional) is **2)**. By putting test_data_sets in the \"output\" folder instead of the \"input\" folder, this approach avoids copying files in the backend. The folder structure will be as below:\n",
    "\n",
    "    - your_input_folder\n",
    "       - model_file(s)\n",
    "    - your_output_folder_to_hold_onnx_file\n",
    "       - test_data_set_0\n",
    "           - input_0.pb\n",
    "           - ...\n",
    "           - output_0.pb\n",
    "           - ...\n",
    "       - test_data_set_1\n",
    "           - ...\n",
    "       ...\n",
    "       - (your .onnx file after running \"onnx-converter\")\n",
    "\n",
    "\n",
    "\n",
    "#### [OPTIONAL] Convert Test Data to ONNX pb \n",
    "\n",
    "ONNX .pb files are expected for test data. If you're more familiar with pickle files, we provide a convenient script to convert your pickle data to pb. Dump your input data to a single pickle file in the following dict format - \n",
    "\n",
    "    {\n",
    "        \"input_name_0\": input_data_0,\n",
    "        \"input_name_1\": input_data_1, \n",
    "        ...\n",
    "    }\n",
    "    \n",
    "or if dumping output data - \n",
    "\n",
    "    {\n",
    "        \"output_name_0\": output_data_0,\n",
    "        \"output_name_1\": output_data_1, \n",
    "        ...\n",
    "    }\n",
    "\n",
    "Then use the [convert_test_data.py](https://github.com/microsoft/OLive/blob/master/utils/convert_test_data.py) to convert your pickle file to pb files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [................................................................................] 2694 / 2694Downloaded scripts\\convert_test_data.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wget\n",
    "\n",
    "# Download convert_test_data.py\n",
    "url = \"https://raw.githubusercontent.com/microsoft/OLive/master/utils/convert_test_data.py\"\n",
    "script_dir = \"scripts\"\n",
    "if not os.path.exists(script_dir):\n",
    "    os.makedirs(script_dir)\n",
    "\n",
    "script_file = os.path.join(script_dir, 'convert_test_data.py')\n",
    "if os.path.exists(script_file):\n",
    "    os.remove(script_file)\n",
    "wget.download(url, script_file)\n",
    "print(\"Downloaded\", script_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run convert_test_data.py to convert your pickle file. This script will read your pickle file and dump the data to a folder named \"test_data_set_0\" by default. Note that ONNX naming convention for test data folder is \"test_data_*\". Make sure to pass `--output_folder` with a folder name starting with `test_data_`. \n",
    "\n",
    "If `--is_input=True`, data will be generated to `input_*.pb`s. Set `--is_input` to false if you'd like to generate output pbs, in which data will be generated to `output_*.pb`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the file specified.\n"
     ]
    }
   ],
   "source": [
    "!./scripts/convert_test_data.py <your_input_pickle_file> --output_folder <output_folder (/test_data_set_0)> --is_input=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you're ready to use OLive to convert model and tune performance. In this tutorial, we'll use [MNIST model from ONNX model zoo](https://github.com/onnx/models/tree/master/vision/classification/mnist) as an example to demo the OLive pipeline. Below are some code to download the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully downloaded and extracted in  test_models\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Download and store the model to the desire directory. Modify these lines to point to your local model directory \n",
    "model_url = \"https://onnxzoo.blob.core.windows.net/models/opset_8/mnist/mnist.tar.gz\"\n",
    "model_dir = \"test_models\"\n",
    "absolute_model_dir = os.path.abspath(model_dir)\n",
    "if not os.path.exists(model_dir):\n",
    "    print(\"Creating model directory \", model_dir)\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "import tarfile\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "file_tmp = urlretrieve(model_url, filename=None)[0]\n",
    "\n",
    "tar = tarfile.open(file_tmp)\n",
    "tar.extractall(model_dir)\n",
    "print(\"Model successfully downloaded and extracted in \", model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run `onnx-converter` Docker Image\n",
    "\n",
    "For the docker image to access your model and test data, you'll need to mount a local directory to docker using the docker parameter `-v`. By using `-v <your_local_directory>:/mnt`, you will be able to share all your files under `<your_local_directory>` to the `/mnt` folder in a running Docker container. Note `your_local_directory` has to be an absolute path. Detailed concept and usage are explained [here](https://docs.docker.com/engine/reference/run/#volume-shared-filesystems)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder shared with docker is  D:\\OLive\\notebook\\test_models\n",
      "Converted ONNX model will be stored at  D:\\OLive\\notebook\\test_models\\output/model.onnx\n"
     ]
    }
   ],
   "source": [
    "# Get the absolute model dir for the local directory to share\n",
    "shared_dir = os.path.abspath(model_dir)\n",
    "# Your input model relative to absolute_model_dir\n",
    "input_model = \"mnist/model.onnx\"\n",
    "# Specify the output folder and converted model name (relative to shared_dir)\n",
    "output_onnx_path = \"output/model.onnx\"\n",
    "# Change model_type to tensorflow, pytorchcntk, coreml, keras or scikit-learn\n",
    "model_type = \"onnx\"\n",
    "print(\"Folder shared with docker is \", shared_dir)\n",
    "print(\"Converted ONNX model will be stored at \", os.path.join(shared_dir, output_onnx_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For different model framework, different parameters are needed for the `onnx-converter` to run. Detailed information on what parameters are needed for your specific model framework, please check onnx-converter [README.md](https://github.com/microsoft/OLive/blob/master/docker-images/onnx-converter/README.md).\n",
    "\n",
    "You can also add a `--test_data_path` parameter to specify your own test data folder (the parent folder to your \"test_data_\\*\" folders) if your test data lie in neither the directory of your input model nor your `--output_onnx_path`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------\n",
      "Model Conversion\n",
      "\n",
      "Input model is already ONNX model. Skipping conversion.\n",
      "\n",
      "-------------\n",
      "MODEL INPUT GENERATION(if needed)\n",
      "\n",
      "Test data .pb files found under /mnt/mnist/test_data_set_0. \n",
      "copying /mnt/mnist/test_data_set_0 to /mnt/output/test_data_set_0\n",
      "Test data .pb files found under /mnt/mnist/test_data_set_1. \n",
      "copying /mnt/mnist/test_data_set_1 to /mnt/output/test_data_set_1\n",
      "Test data .pb files found under /mnt/mnist/test_data_set_2. \n",
      "copying /mnt/mnist/test_data_set_2 to /mnt/output/test_data_set_2\n",
      "Test data .pb files already exist. Skipping dummy input generation. \n",
      "\n",
      "-------------\n",
      "MODEL CORRECTNESS VERIFICATION\n",
      "\n",
      "\n",
      "Check the ONNX model for validity \n",
      "The ONNX model is valid.\n",
      "\n",
      "The original model is already onnx. Skipping correctness test. \n",
      "\n",
      "-------------\n",
      "MODEL CONVERSION SUMMARY (.json file generated at /mnt/output/output.json )\n",
      "\n",
      "{'conversion_status': 'SUCCESS',\n",
      " 'correctness_verified': 'SKIPPED',\n",
      " 'error_message': '',\n",
      " 'input_folder': '/mnt/output/test_data_set_0',\n",
      " 'output_onnx_path': '/mnt/output/model.onnx'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:scikit-learn version 0.24.1 is not supported. Minimum required version: 0.17. Maximum required version: 0.19.2. Disabling scikit-learn conversion API.\n"
     ]
    }
   ],
   "source": [
    "!docker run -v {shared_dir}:/mnt \\\n",
    "    mcr.microsoft.com/onnxruntime/onnx-converter \\\n",
    "    --model /mnt/{input_model} \\\n",
    "    --output_onnx_path /mnt/{output_onnx_path} \\\n",
    "    --model_type {model_type}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the converted ONNX model\n",
    "\n",
    "By using Netron model visualization tool, we can check out the newly converted ONNX model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'D:\\OLive\\notebook\\test_models\\output/model.onnx' at http://localhost:8080\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"1000\"\n",
       "            src=\"http://localhost:8080\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1dfdc1346c8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import netron\n",
    "netron.start(os.path.join(shared_dir, output_onnx_path), browse=False)\n",
    "from IPython.display import IFrame\n",
    "IFrame('http://localhost:8080', width=\"100%\", height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopping http://localhost:8080\n"
     ]
    }
   ],
   "source": [
    "# Stop Netron server\n",
    "netron.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your model has been successfully converted. The model file and its test data are stored in the `output_onnx_path` folder. The files in this folder are ready to serve as inputs for the following `perf-tuning` step. \n",
    "\n",
    "## 2. Performance Tuning\n",
    "\n",
    "The `perf-tuning` docker image tunes the best settings for an ONNX model to run in ONNX Runtime. It sweeps combinations of threads, environment variables, and execution providers for the best performance numbers. Top 3 results will be rendered for each available execution provider.  \n",
    "\n",
    "### Prepare inputs\n",
    "\n",
    "You'll need to store your ONNX model file as well as its test data in the same folder in the following structure - \n",
    "\n",
    "    -- local_directory_to_your_models\n",
    "        -- ModelDir\n",
    "            -- model.onnx\n",
    "            -- test_data_set_0\n",
    "                -- input_0.pb\n",
    "                -- output_0.pb\n",
    "                -- ...\n",
    "            -- test_data_set_1            \n",
    "                -- ...\n",
    "            -- ...\n",
    "            \n",
    "Test data is required in this step. If you follow the conversion step in the notebook to this point, the output folder for the `onnx-converter` is already in this folder structure and can be directly used as input for `perf-tuning`.\n",
    "\n",
    "### Run `perf-tuning` Docker Image\n",
    "\n",
    "\n",
    "A few things to note when running the `perf-tuning` Docker image: \n",
    "\n",
    " - Currently supported execution providers(EPs) are cpu, cpu_openmp, dnnl, mklml, cuda, tensorrt, ngraph, and nuphar. Add `-e cpu,dnnl,...`(no spaces between the EPs) to select the execution providers you'd like to tune. By default, all available EPs will be tuned.\n",
    " - EPs such as CUDA and TensorRT require GPU. To use those EPs, make sure you have GPU in your local machine, and add `--gpus all` to Docker (BEFORE `mcr.microsoft.com/onnxruntime/perf-tuning`) to leverage your GPUs. Otherwise GPU based execution providers will be skipped. \n",
    " - Just like `onnx-converter`, `perf-tuning` also needs users to share their local directories to the Docker container using the `-v` command. \n",
    " - Other available commands for `perf-tuning` are documented [here](https://github.com/microsoft/OLive/tree/master/docker-images/perf-tuning). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some variables \n",
    "onnx_model = output_onnx_path        # reuse output of the \"convert\" step here. Adjust as neccessary\n",
    "result_dir = \"result\"             # output folder to hold your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting inter_op_num_threads to 2\n",
      "Session creation time cost: 0.0042849 s\n",
      "Total inference time cost: 0.0012188 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.06094 ms\n",
      "Total inference run time: 0.0012469 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 16519168 bytes\n",
      "Setting inter_op_num_threads to 7\n",
      "Session creation time cost: 0.0061397 s\n",
      "Total inference time cost: 0.0189784 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.94892 ms\n",
      "Total inference run time: 0.0190204 s\n",
      "Avg CPU usage: 54 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-13 00:25:40.111126200 [E:onnxruntime:Default, provider_bridge_ort.cc:572 Ensure] Failed to load library libonnxruntime_providers_shared.so with error: libonnxruntime_providers_shared.so: cannot open shared object file: No such file or directory\n",
      "OrtSessionOptionsAppendExecutionProvider_Tensorrt: Failed to load shared library\n",
      "2021-03-13 00:25:40.154288800 [E:onnxruntime:Default, provider_bridge_ort.cc:572 Ensure] Failed to load library libonnxruntime_providers_shared.so with error: libonnxruntime_providers_shared.so: cannot open shared object file: No such file or directory\n",
      "OrtSessionOptionsAppendExecutionProvider_Tensorrt: Failed to load shared library\n",
      "/bert_ort/ziyl/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:123 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] /bert_ort/ziyl/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:117 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] CUDA failure 35: CUDA driver version is insufficient for CUDA runtime version ; GPU=32544 ; hostname=46089f78b4e2 ; expr=cudaSetDevice(info_.device_id); \n",
      "\n",
      "\n",
      "/bert_ort/ziyl/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:123 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] /bert_ort/ziyl/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:117 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] CUDA failure 35: CUDA driver version is insufficient for CUDA runtime version ; GPU=32657 ; hostname=46089f78b4e2 ; expr=cudaSetDevice(info_.device_id); \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak working set size: 16572416 bytes\n",
      "Setting inter_op_num_threads to 4\n",
      "Session creation time cost: 0.0045842 s\n",
      "Total inference time cost: 0.0014224 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.07112 ms\n",
      "Total inference run time: 0.001472 s\n",
      "Avg CPU usage: 8 %\n",
      "Peak working set size: 17043456 bytes\n",
      "Setting inter_op_num_threads to 3\n",
      "Session creation time cost: 0.0053919 s\n",
      "Total inference time cost: 0.0017238 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.08619 ms\n",
      "Total inference run time: 0.0017566 s\n",
      "Avg CPU usage: 8 %\n",
      "Peak working set size: 17108992 bytes\n",
      "Session creation time cost: 0.0044967 s\n",
      "Total inference time cost: 0.0006097 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.030485 ms\n",
      "Total inference run time: 0.0006278 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 16646144 bytes\n",
      "Session creation time cost: 0.0042654 s\n",
      "Total inference time cost: 0.0006185 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.030925 ms\n",
      "Total inference run time: 0.0006323 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 16572416 bytes\n",
      "Session creation time cost: 0.0053419 s\n",
      "Total inference time cost: 0.0008197 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.040985 ms\n",
      "Total inference run time: 0.0008373 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 16654336 bytes\n",
      "Session creation time cost: 0.0042057 s\n",
      "Total inference time cost: 0.0006083 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.030415 ms\n",
      "Total inference run time: 0.0006206 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 16732160 bytes\n",
      "Session creation time cost: 0.0047835 s\n",
      "Total inference time cost: 0.0006939 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.034695 ms\n",
      "Total inference run time: 0.0007068 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 16711680 bytes\n",
      "Session creation time cost: 0.0046791 s\n",
      "Total inference time cost: 0.0006255 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.031275 ms\n",
      "Total inference run time: 0.0006388 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 18878464 bytes\n",
      "Session creation time cost: 0.0050919 s\n",
      "Total inference time cost: 0.0008125 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.040625 ms\n",
      "Total inference run time: 0.0008343 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 16474112 bytes\n",
      "Session creation time cost: 0.0042122 s\n",
      "Total inference time cost: 0.0006536 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.03268 ms\n",
      "Total inference run time: 0.0006667 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 16551936 bytes\n",
      "Session creation time cost: 0.0047356 s\n",
      "Total inference time cost: 0.0008206 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.04103 ms\n",
      "Total inference run time: 0.0008398 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 16666624 bytes\n",
      "Setting inter_op_num_threads to 2\n",
      "Session creation time cost: 0.0089357 s\n",
      "Total inference time cost: 0.0014105 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.070525 ms\n",
      "Total inference run time: 0.0014479 s\n",
      "Avg CPU usage: 0 %\n",
      "Peak working set size: 16347136 bytes\n",
      "Session creation time cost: 0.0044615 s\n",
      "Total inference time cost: 0.0006057 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.030285 ms\n",
      "Total inference run time: 0.0006229 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 16474112 bytes\n",
      "Setting inter_op_num_threads to 2\n",
      "Session creation time cost: 0.0043403 s\n",
      "Total inference time cost: 0.0011997 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.059985 ms\n",
      "Total inference run time: 0.0012274 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 16519168 bytes\n",
      "Session creation time cost: 0.0043585 s\n",
      "Total inference time cost: 0.0006606 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.03303 ms\n",
      "Total inference run time: 0.0006749 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 16658432 bytes\n",
      "Setting inter_op_num_threads to 2\n",
      "Session creation time cost: 0.0087674 s\n",
      "Total inference time cost: 0.0363829 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 1.81914 ms\n",
      "Total inference run time: 0.0364288 s\n",
      "Avg CPU usage: 18 %\n",
      "Peak working set size: 51580928 bytes\n",
      "Setting inter_op_num_threads to 7\n",
      "Session creation time cost: 0.0107283 s\n",
      "Total inference time cost: 0.0591921 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 2.9596 ms\n",
      "Total inference run time: 0.0592489 s\n",
      "Avg CPU usage: 38 %\n",
      "Peak working set size: 55508992 bytes\n",
      "Setting inter_op_num_threads to 4\n",
      "Session creation time cost: 0.0086098 s\n",
      "Total inference time cost: 0.0537621 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 2.68811 ms\n",
      "Total inference run time: 0.0538227 s\n",
      "Avg CPU usage: 25 %\n",
      "Peak working set size: 52097024 bytes\n",
      "Setting inter_op_num_threads to 3\n",
      "Session creation time cost: 0.0096317 s\n",
      "Total inference time cost: 0.0406705 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 2.03353 ms\n",
      "Total inference run time: 0.0407363 s\n",
      "Avg CPU usage: 31 %\n",
      "Peak working set size: 58327040 bytes\n",
      "Session creation time cost: 0.0081725 s\n",
      "Total inference time cost: 0.0007963 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.039815 ms\n",
      "Total inference run time: 0.0008182 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 50016256 bytes\n",
      "Session creation time cost: 0.0055591 s\n",
      "Total inference time cost: 0.0007946 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.03973 ms\n",
      "Total inference run time: 0.0008116 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 49057792 bytes\n",
      "Session creation time cost: 0.0070671 s\n",
      "Total inference time cost: 0.0009212 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.04606 ms\n",
      "Total inference run time: 0.0009383 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 51892224 bytes\n",
      "Session creation time cost: 0.0063797 s\n",
      "Total inference time cost: 0.0008528 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.04264 ms\n",
      "Total inference run time: 0.0008721 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 50057216 bytes\n",
      "Session creation time cost: 0.0063618 s\n",
      "Total inference time cost: 0.0074705 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.373525 ms\n",
      "Total inference run time: 0.0074878 s\n",
      "Avg CPU usage: 8 %\n",
      "Peak working set size: 50065408 bytes\n",
      "Session creation time cost: 0.0121813 s\n",
      "Total inference time cost: 0.030088 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 1.5044 ms\n",
      "Total inference run time: 0.0301251 s\n",
      "Avg CPU usage: 19 %\n",
      "Peak working set size: 49975296 bytes\n",
      "Session creation time cost: 0.0130363 s\n",
      "Total inference time cost: 0.0334128 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 1.67064 ms\n",
      "Total inference run time: 0.0334779 s\n",
      "Avg CPU usage: 19 %\n",
      "Peak working set size: 49438720 bytes\n",
      "Session creation time cost: 0.0094133 s\n",
      "Total inference time cost: 0.022409 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 1.12045 ms\n",
      "Total inference run time: 0.0224458 s\n",
      "Avg CPU usage: 16 %\n",
      "Peak working set size: 50135040 bytes\n",
      "Session creation time cost: 0.0132957 s\n",
      "Total inference time cost: 0.0026311 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.131555 ms\n",
      "Total inference run time: 0.0026807 s\n",
      "Avg CPU usage: 50 %\n",
      "Peak working set size: 51834880 bytes\n",
      "Setting inter_op_num_threads to 2\n",
      "Session creation time cost: 0.0055714 s\n",
      "Total inference time cost: 0.019921 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.99605 ms\n",
      "Total inference run time: 0.0199581 s\n",
      "Avg CPU usage: 70 %\n",
      "Peak working set size: 51671040 bytes\n",
      "Session creation time cost: 0.0349188 s\n",
      "Total inference time cost: 0.0010141 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.050705 ms\n",
      "Total inference run time: 0.0010311 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 51716096 bytes\n",
      "Cores:  12\n",
      "No GPU found on current device. Cuda and TensorRT performance tuning might not be available. \n",
      "providers  ['cpu_openmp', 'dnnl', 'cpu', 'tensorrt', 'openvino', 'cuda', 'nuphar']\n",
      "\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/cpu/onnxruntime_perf_test -P -y 2 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/1dc72d5c-8a54-418e-8a84-b69173f2bd92\n",
      "cpu_openmp_parallel_2_inter_threads 0.06094\n",
      "\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/cpu/onnxruntime_perf_test -P -y 7 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/cba55af8-207f-44c3-b0ba-ea67820f7cbd\n",
      "cpu_openmp_parallel_7_inter_threads 0.94892\n",
      "\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/cpu/onnxruntime_perf_test -P -y 4 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/f1db8d24-1511-4735-8a21-9f07b45fb18e\n",
      "cpu_openmp_parallel_4_inter_threads 0.07112\n",
      "\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/cpu/onnxruntime_perf_test -P -y 3 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/3b96ac4f-ba22-485f-9f51-61ddf5e9f56f\n",
      "cpu_openmp_parallel_3_inter_threads 0.08619\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "OMP_NUM_THREADS=2\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/cpu/onnxruntime_perf_test -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/19708923-e6ac-4514-a32c-404edf27d935\n",
      "cpu_openmp_2_OMP_threads_OMP_WAIT_POLICY_active 0.030485\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "OMP_NUM_THREADS=7\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/cpu/onnxruntime_perf_test -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/fccc6368-5b9e-444f-8c1f-12880ab72208\n",
      "cpu_openmp_7_OMP_threads_OMP_WAIT_POLICY_active 0.030925\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "OMP_NUM_THREADS=4\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/cpu/onnxruntime_perf_test -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/a340ff61-56b4-4a6d-9665-2ae6b1eb453b\n",
      "cpu_openmp_4_OMP_threads_OMP_WAIT_POLICY_active 0.040985\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "OMP_NUM_THREADS=3\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/cpu/onnxruntime_perf_test -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/03db09e7-8e63-4ba2-b50d-4771552177f4\n",
      "cpu_openmp_3_OMP_threads_OMP_WAIT_POLICY_active 0.030414999999999998\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "OMP_NUM_THREADS=2\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/cpu/onnxruntime_perf_test -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/fbddc211-b3ea-44e0-8da9-7967ae01dd85\n",
      "cpu_openmp_2_OMP_threads_OMP_WAIT_POLICY_passive 0.034695\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "OMP_NUM_THREADS=7\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/cpu/onnxruntime_perf_test -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/22e1e9bf-37ec-4241-ab36-f85a64fc7827\n",
      "cpu_openmp_7_OMP_threads_OMP_WAIT_POLICY_passive 0.031275000000000004\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "OMP_NUM_THREADS=10\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/cpu/onnxruntime_perf_test -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/649f6f02-0932-4742-88e8-44850bf1c82d\n",
      "cpu_openmp_10_OMP_threads_OMP_WAIT_POLICY_passive 0.040625\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "OMP_NUM_THREADS=8\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/cpu/onnxruntime_perf_test -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/a02e5b20-8f25-42db-b1d8-0ccdde8e1304\n",
      "cpu_openmp_8_OMP_threads_OMP_WAIT_POLICY_passive 0.03268\n",
      "\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/cpu/onnxruntime_perf_test -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/3caf475e-2fac-4b69-9d63-23b33ea00698\n",
      "cpu_openmp 0.04103\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "OMP_NUM_THREADS=3\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/cpu/onnxruntime_perf_test -P -y 2 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/23813555-d674-420c-8a46-f88a26870963\n",
      "cpu_openmp_parallel_3_OMP_threads_2_inter_threads_OMP_WAIT_POLICY_active 0.070525\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/cpu/onnxruntime_perf_test -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/9e3ae247-4ba4-4a9b-9c55-4c984446addf\n",
      "cpu_openmp_OMP_WAIT_POLICY_active 0.030285\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "OMP_NUM_THREADS=7\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/cpu/onnxruntime_perf_test -P -y 2 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/9d4f87b2-c2d3-4764-ab22-c8067345bfbb\n",
      "cpu_openmp_parallel_7_OMP_threads_2_inter_threads_OMP_WAIT_POLICY_passive 0.059985000000000004\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/cpu/onnxruntime_perf_test -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/ffafefa8-0206-44b7-a7c7-379b7c8e6cc9\n",
      "cpu_openmp_OMP_WAIT_POLICY_passive 0.033030000000000004\n",
      "\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -P -y 2 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/65744e53-9a68-47c2-a0c0-75e79b1d7fed\n",
      "dnnl_parallel_2_inter_threads 1.819145\n",
      "\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -P -y 7 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/b58e31f0-59b3-4451-ba7b-bbc280e99632\n",
      "dnnl_parallel_7_inter_threads 2.9596050000000003\n",
      "\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -P -y 4 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/ea4eab0e-14fe-4122-8344-6f962db61332\n",
      "dnnl_parallel_4_inter_threads 2.688105\n",
      "\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -P -y 3 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/7089b854-2360-46b7-b96e-327167d832af\n",
      "dnnl_parallel_3_inter_threads 2.033525\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "OMP_NUM_THREADS=2\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/fa5d1784-d5c1-40a8-880a-6ee47a0c6fb9\n",
      "dnnl_2_OMP_threads_OMP_WAIT_POLICY_active 0.039814999999999996\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "OMP_NUM_THREADS=7\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/c54f7820-2b8f-4629-8660-8c1233949a0c\n",
      "dnnl_7_OMP_threads_OMP_WAIT_POLICY_active 0.03973\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "OMP_NUM_THREADS=10\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/fcfde05c-9658-4226-a9f5-f7ce06554342\n",
      "dnnl_10_OMP_threads_OMP_WAIT_POLICY_active 0.046060000000000004\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "OMP_NUM_THREADS=8\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/b11662ee-c6f5-4e1f-a056-d5be309382e8\n",
      "dnnl_8_OMP_threads_OMP_WAIT_POLICY_active 0.04264\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "OMP_NUM_THREADS=2\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/ce2ee135-5ac0-4412-a889-c56a5517f8dd\n",
      "dnnl_2_OMP_threads_OMP_WAIT_POLICY_passive 0.373525\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "OMP_NUM_THREADS=7\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/2e5da0c2-2009-4567-86dd-9e0f6bf099b4\n",
      "dnnl_7_OMP_threads_OMP_WAIT_POLICY_passive 1.5044\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "OMP_NUM_THREADS=4\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/ec6fded7-3dc4-4123-a350-65c0dba2805c\n",
      "dnnl_4_OMP_threads_OMP_WAIT_POLICY_passive 1.67064\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "OMP_NUM_THREADS=3\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/3dfb1552-c040-4e61-8cd0-808626109de6\n",
      "dnnl_3_OMP_threads_OMP_WAIT_POLICY_passive 1.12045\n",
      "\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/8cc2a3df-bcde-48be-a255-8eac8985c875\n",
      "dnnl 0.131555\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "OMP_NUM_THREADS=7\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -P -y 2 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/1c598f78-ff61-4d0c-96ad-0bf312e6863a\n",
      "dnnl_parallel_7_OMP_threads_2_inter_threads_OMP_WAIT_POLICY_active 0.9960500000000001\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/69c76e73-b99f-48e1-b7b8-7e25a59bd3cd\n",
      "dnnl_OMP_WAIT_POLICY_active 0.050705\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "OMP_NUM_THREADS=2\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -P -y 2 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/19985e23-0eb6-4955-a6ee-ae6472bebdb3Setting inter_op_num_threads to 2\n",
      "Session creation time cost: 0.0108934 s\n",
      "Total inference time cost: 0.0066348 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.33174 ms\n",
      "Total inference run time: 0.0066659 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 49688576 bytes\n",
      "Session creation time cost: 0.0082342 s\n",
      "Total inference time cost: 0.0920827 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 4.60414 ms\n",
      "Total inference run time: 0.0921579 s\n",
      "Avg CPU usage: 21 %\n",
      "Peak working set size: 50196480 bytes\n",
      "Setting inter_op_num_threads to 2\n",
      "Session creation time cost: 0.0043933 s\n",
      "Total inference time cost: 0.0011843 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.059215 ms\n",
      "Total inference run time: 0.0012062 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 16408576 bytes\n",
      "Setting inter_op_num_threads to 7\n",
      "Session creation time cost: 0.0062311 s\n",
      "Total inference time cost: 0.0016014 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.08007 ms\n",
      "Total inference run time: 0.0016252 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 16781312 bytes\n",
      "Setting inter_op_num_threads to 4\n",
      "Session creation time cost: 0.0042132 s\n",
      "Total inference time cost: 0.0013184 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.06592 ms\n",
      "Total inference run time: 0.0013393 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 18956288 bytes\n",
      "Setting inter_op_num_threads to 3\n",
      "Session creation time cost: 0.0044681 s\n",
      "Total inference time cost: 0.0012692 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.06346 ms\n",
      "Total inference run time: 0.0012933 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 16412672 bytes\n",
      "Setting intra_op_num_threads to 2\n",
      "Session creation time cost: 0.0049378 s\n",
      "Total inference time cost: 0.0012728 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.06364 ms\n",
      "Total inference run time: 0.0013076 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 16904192 bytes\n",
      "Setting intra_op_num_threads to 7\n",
      "Session creation time cost: 0.0054869 s\n",
      "Total inference time cost: 0.000609 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.03045 ms\n",
      "Total inference run time: 0.0006212 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 16588800 bytes\n",
      "Setting intra_op_num_threads to 9\n",
      "Session creation time cost: 0.0043608 s\n",
      "Total inference time cost: 0.0007773 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.038865 ms\n",
      "Total inference run time: 0.0007938 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 16945152 bytes\n",
      "Setting intra_op_num_threads to 8\n",
      "Session creation time cost: 0.0042245 s\n",
      "Total inference time cost: 0.0007184 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.03592 ms\n",
      "Total inference run time: 0.000736 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 17063936 bytes\n",
      "Setting intra_op_num_threads to 7\n",
      "Setting inter_op_num_threads to 2\n",
      "Session creation time cost: 0.0053634 s\n",
      "Total inference time cost: 0.0015267 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.076335 ms\n",
      "Total inference run time: 0.0015591 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 17133568 bytes\n",
      "Session creation time cost: 0.004098 s\n",
      "Total inference time cost: 0.0006313 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.031565 ms\n",
      "Total inference run time: 0.0006454 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 16715776 bytes\n",
      "Session creation time cost: 0.0548773 s\n",
      "Total inference time cost: 0.0064927 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.324635 ms\n",
      "Total inference run time: 0.0065401 s\n",
      "Avg CPU usage: 33 %\n",
      "Peak working set size: 68009984 bytes\n",
      "Session creation time cost: 0.025188 s\n",
      "Total inference time cost: 0.0038017 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.190085 ms\n",
      "Total inference run time: 0.0038345 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 65699840 bytes\n",
      "Session creation time cost: 0.0282163 s\n",
      "Total inference time cost: 0.002284 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.1142 ms\n",
      "Total inference run time: 0.0023184 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 67592192 bytes\n",
      "Session creation time cost: 0.0414557 s\n",
      "Total inference time cost: 0.0029326 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.14663 ms\n",
      "Total inference run time: 0.002961 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 66502656 bytes\n",
      "Session creation time cost: 0.0597454 s\n",
      "Total inference time cost: 0.0089235 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.446175 ms\n",
      "Total inference run time: 0.0089969 s\n",
      "Avg CPU usage: 25 %\n",
      "Peak working set size: 67739648 bytes\n",
      "Session creation time cost: 0.039032 s\n",
      "Total inference time cost: 0.0098804 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.49402 ms\n",
      "Total inference run time: 0.0099648 s\n",
      "Avg CPU usage: 33 %\n",
      "Peak working set size: 66478080 bytes\n",
      "Session creation time cost: 0.0477974 s\n",
      "Total inference time cost: 0.0092876 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.46438 ms\n",
      "Total inference run time: 0.0093679 s\n",
      "Avg CPU usage: 16 %\n",
      "Peak working set size: 70578176 bytes\n",
      "Session creation time cost: 0.0299327 s\n",
      "Total inference time cost: 0.0039948 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.19974 ms\n",
      "Total inference run time: 0.0040343 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 67772416 bytes\n",
      "Session creation time cost: 0.032368 s\n",
      "Total inference time cost: 0.004586 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.2293 ms\n",
      "Total inference run time: 0.0046323 s\n",
      "Avg CPU usage: 8 %\n",
      "Peak working set size: 66207744 bytes\n",
      "Session creation time cost: 0.0672226 s\n",
      "Total inference time cost: 0.0037768 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.18884 ms\n",
      "Total inference run time: 0.0038119 s\n",
      "Avg CPU usage: 75 %\n",
      "Peak working set size: 75096064 bytes\n",
      "Session creation time cost: 0.030506 s\n",
      "Total inference time cost: 0.0077177 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.385885 ms\n",
      "Total inference run time: 0.0077654 s\n",
      "Avg CPU usage: 16 %\n",
      "Peak working set size: 65720320 bytes\n",
      "Setting intra_op_num_threads to 2\n",
      "Session creation time cost: 0.100994 s\n",
      "Total inference time cost: 0.0006838 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.03419 ms\n",
      "Total inference run time: 0.0006962 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 61374464 bytes\n",
      "Session creation time cost: 0.131866 s\n",
      "Total inference time cost: 0.0006108 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.03054 ms\n",
      "Total inference run time: 0.0006245 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 61239296 bytes\n",
      "Session creation time cost: 0.14179 s\n",
      "Total inference time cost: 0.0007448 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.03724 ms\n",
      "Total inference run time: 0.0007638 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 61333504 bytes\n",
      "Session creation time cost: 0.11362 s\n",
      "Total inference time cost: 0.0007682 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.03841 ms\n",
      "Total inference run time: 0.000787 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 61407232 bytes\n",
      "Session creation time cost: 0.102504 s\n",
      "Total inference time cost: 0.0066689 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.333445 ms\n",
      "Total inference run time: 0.006707 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 61521920 bytes\n",
      "Session creation time cost: 0.114719 s\n",
      "Total inference time cost: 0.0363658 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 1.81829 ms\n",
      "Total inference run time: 0.0364111 s\n",
      "Avg CPU usage: 18 %\n",
      "Peak working set size: 61632512 bytes\n",
      "Session creation time cost: 0.163987 s\n",
      "Total inference time cost: 0.0197569 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.987845 ms\n",
      "Total inference run time: 0.0197967 s\n",
      "Avg CPU usage: 16 %\n",
      "Peak working set size: 60760064 bytes\n",
      "Session creation time cost: 0.120923 s\n",
      "Total inference time cost: 0.0153459 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.767295 ms\n",
      "Total inference run time: 0.015397 s\n",
      "Avg CPU usage: 12 %\n",
      "Peak working set size: 61235200 bytes\n",
      "\n",
      "dnnl_parallel_2_OMP_threads_2_inter_threads_OMP_WAIT_POLICY_passive 0.33174000000000003\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/190cfda4-69f7-4905-98bc-fce7aeabac2b\n",
      "dnnl_OMP_WAIT_POLICY_passive 4.604134999999999\n",
      "\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/cpu/onnxruntime_perf_test -e cpu -P -y 2 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/764bfce1-6a1b-4f26-83b9-b93bd452d0dd\n",
      "cpu_parallel_2_inter_threads 0.059215000000000004\n",
      "\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/cpu/onnxruntime_perf_test -e cpu -P -y 7 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/13212ccd-f91c-4f10-96a7-50a9f8b74286\n",
      "cpu_parallel_7_inter_threads 0.08007\n",
      "\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/cpu/onnxruntime_perf_test -e cpu -P -y 4 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/d5830eb6-10ca-4f80-a132-cdbe1039c4a7\n",
      "cpu_parallel_4_inter_threads 0.06592\n",
      "\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/cpu/onnxruntime_perf_test -e cpu -P -y 3 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/f1ea7378-3dc5-4f46-b5ee-12012f70536f\n",
      "cpu_parallel_3_inter_threads 0.06346\n",
      "\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/cpu/onnxruntime_perf_test -e cpu -x 2 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/5af1ea6d-a04d-462a-b0a7-96c4a15af6ba\n",
      "cpu_2_intra_threads 0.06363999999999999\n",
      "\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/cpu/onnxruntime_perf_test -e cpu -x 7 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/7b58dff6-70c3-451f-9800-781bb4154efb\n",
      "cpu_7_intra_threads 0.03045\n",
      "\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/cpu/onnxruntime_perf_test -e cpu -x 9 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/df274047-55be-4b71-8b2b-9ae07a31e09e\n",
      "cpu_9_intra_threads 0.038865000000000004\n",
      "\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/cpu/onnxruntime_perf_test -e cpu -x 8 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/37b87b27-b70b-4e21-9b05-93b6daab5b17\n",
      "cpu_8_intra_threads 0.03592\n",
      "\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/cpu/onnxruntime_perf_test -e cpu -P -y 2 -x 7 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/54bdc38d-15c9-448c-ab73-e8ce58c6de52\n",
      "cpu_parallel_7_intra_threads_2_inter_threads 0.076335\n",
      "\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/cpu/onnxruntime_perf_test -e cpu -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/6785c013-6b17-4357-8945-75a4c03a7c64\n",
      "cpu 0.031565\n",
      "\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/gpu/onnxruntime_perf_test -e tensorrt -x 2 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/0c090c03-8935-4da2-ac7d-5fea3e82e394\n",
      "tensorrt_2_intra_threads None\n",
      "\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/gpu/onnxruntime_perf_test -e tensorrt -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/1e3fe7ae-42fb-4c78-a103-84446a01bf88\n",
      "tensorrt None\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "OMP_NUM_THREADS=2\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e openvino -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/b4b77f56-6397-44fa-809d-7d9afc22b088\n",
      "openvino_2_OMP_threads_OMP_WAIT_POLICY_active 0.324635\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "OMP_NUM_THREADS=7\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e openvino -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/fde76bf7-4058-4e1a-b7ca-acee57ac3d15\n",
      "openvino_7_OMP_threads_OMP_WAIT_POLICY_active 0.190085\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "OMP_NUM_THREADS=10\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e openvino -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/7306a9dc-4139-4c86-a5af-8d2237c3d611\n",
      "openvino_10_OMP_threads_OMP_WAIT_POLICY_active 0.1142\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "OMP_NUM_THREADS=11\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e openvino -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/fae513e1-bcb3-4274-9af6-6678d4e62f3a\n",
      "openvino_11_OMP_threads_OMP_WAIT_POLICY_active 0.14663\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "OMP_NUM_THREADS=2\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e openvino -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/04c2e4c0-0c88-4524-8a4e-dd084365d059\n",
      "openvino_2_OMP_threads_OMP_WAIT_POLICY_passive 0.446175\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "OMP_NUM_THREADS=7\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e openvino -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/34939cd7-23ab-4523-bac8-7e54f6e3f070\n",
      "openvino_7_OMP_threads_OMP_WAIT_POLICY_passive 0.49401999999999996\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "OMP_NUM_THREADS=4\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e openvino -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/f6aef3ea-1f41-4c6c-9be1-c8dbfafb8458\n",
      "openvino_4_OMP_threads_OMP_WAIT_POLICY_passive 0.46438\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "OMP_NUM_THREADS=3\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e openvino -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/95a98d1e-c678-4d80-a74e-3de659b7210c\n",
      "openvino_3_OMP_threads_OMP_WAIT_POLICY_passive 0.19974\n",
      "\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e openvino -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/bcafc071-3bcf-4fad-bca2-e5a06d4d8368\n",
      "openvino 0.2293\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e openvino -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/e8eb19a1-cbf6-4b2c-a334-c7e9c7de0813\n",
      "openvino_OMP_WAIT_POLICY_active 0.18883999999999998\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e openvino -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/4a798ed3-e42e-435a-b613-173c870022ef\n",
      "openvino_OMP_WAIT_POLICY_passive 0.385885\n",
      "\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/gpu/onnxruntime_perf_test -e cuda -x 2 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/b6d9124d-7cf7-4b0a-9957-1932b2fbb4f5\n",
      "cuda_2_intra_threads None\n",
      "\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/gpu/onnxruntime_perf_test -e cuda -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/b338e59a-af01-4037-9888-679e7052b4e0\n",
      "cuda None\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "OMP_NUM_THREADS=2\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e nuphar -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/51e01d2b-fd29-42c8-a48e-48815c9e24fc\n",
      "nuphar_2_OMP_threads_OMP_WAIT_POLICY_active 0.034190000000000005\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "OMP_NUM_THREADS=7\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e nuphar -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/0b17ae51-b715-477b-8b88-31df5720f366\n",
      "nuphar_7_OMP_threads_OMP_WAIT_POLICY_active 0.03054\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "OMP_NUM_THREADS=10\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e nuphar -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/14b1d06e-cf8c-41dc-a3da-8ad7bf7402e4\n",
      "nuphar_10_OMP_threads_OMP_WAIT_POLICY_active 0.03724\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "OMP_NUM_THREADS=8\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e nuphar -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/e4d5ce6d-920b-45fd-89f9-205db2705d7f\n",
      "nuphar_8_OMP_threads_OMP_WAIT_POLICY_active 0.03841\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "OMP_NUM_THREADS=2\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e nuphar -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/faf8895b-a82d-44d0-a9f8-222563159a09\n",
      "nuphar_2_OMP_threads_OMP_WAIT_POLICY_passive 0.333445\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "OMP_NUM_THREADS=7\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e nuphar -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/df004cd1-4e5f-4ee5-844b-bd6e293107cf\n",
      "nuphar_7_OMP_threads_OMP_WAIT_POLICY_passive 1.81829\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "OMP_NUM_THREADS=4\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e nuphar -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/571eea3f-b02c-4e8a-8ace-52ffeedc88fc\n",
      "nuphar_4_OMP_threads_OMP_WAIT_POLICY_passive 0.987845\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "OMP_NUM_THREADS=3\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e nuphar -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/34ceedbb-18a3-408a-9dc5-9b3792daa5cb\n",
      "nuphar_3_OMP_threads_OMP_WAIT_POLICY_passiveSession creation time cost: 0.128781 s\n",
      "Total inference time cost: 0.002259 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.11295 ms\n",
      "Total inference run time: 0.0023133 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 61304832 bytes\n",
      "Session creation time cost: 0.178619 s\n",
      "Total inference time cost: 0.0017147 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 0.085735 ms\n",
      "Total inference run time: 0.0017332 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 63688704 bytes\n",
      "Session creation time cost: 0.133472 s\n",
      "Total inference time cost: 0.060462 s\n",
      "Total inference requests: 20\n",
      "Average inference time cost: 3.0231 ms\n",
      "Total inference run time: 0.0605261 s\n",
      "Avg CPU usage: 19 %\n",
      "Peak working set size: 61374464 bytes\n",
      "Session creation time cost: 0.0057247 s\n",
      "Total inference time cost: 0.0079046 s\n",
      "Total inference requests: 200\n",
      "Average inference time cost: 0.039523 ms\n",
      "Total inference run time: 0.0080521 s\n",
      "Avg CPU usage: -1 %\n",
      "Peak working set size: 16576512 bytes\n",
      "Session creation time cost: 0.0057577 s\n",
      "Total inference time cost: 0.0070037 s\n",
      "Total inference requests: 200\n",
      "Average inference time cost: 0.0350185 ms\n",
      "Total inference run time: 0.0070775 s\n",
      "Avg CPU usage: 50 %\n",
      "Peak working set size: 49590272 bytes\n",
      "Setting intra_op_num_threads to 7\n",
      "Session creation time cost: 0.0051631 s\n",
      "Total inference time cost: 0.0073764 s\n",
      "Total inference requests: 200\n",
      "Average inference time cost: 0.036882 ms\n",
      "Total inference run time: 0.0075336 s\n",
      "Avg CPU usage: 50 %\n",
      "Peak working set size: 16408576 bytes\n",
      "Session creation time cost: 0.0597547 s\n",
      "Total inference time cost: 0.0518382 s\n",
      "Total inference requests: 200\n",
      "Average inference time cost: 0.259191 ms\n",
      "Total inference run time: 0.0520765 s\n",
      "Avg CPU usage: 91 %\n",
      "Peak working set size: 66621440 bytes\n",
      "Session creation time cost: 0.140615 s\n",
      "Total inference time cost: 0.0064618 s\n",
      "Total inference requests: 200\n",
      "Average inference time cost: 0.032309 ms\n",
      "Total inference run time: 0.006566 s\n",
      "Avg CPU usage: 58 %\n",
      "Peak working set size: 61612032 bytes\n",
      " 0.767295\n",
      "\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e nuphar -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/5fc8a5a4-dd3d-43e6-8da1-a0eb2030c8ef\n",
      "nuphar 0.11295\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e nuphar -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/8d428f70-dff3-49c0-9ac6-2db8a39550e8\n",
      "nuphar_OMP_WAIT_POLICY_active 0.085735\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e nuphar -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/a0c996c3-538a-4851-9698-495adaf7e4d7\n",
      "nuphar_OMP_WAIT_POLICY_passive 3.0231\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/cpu/onnxruntime_perf_test -o 99 -m times -r 200 /mnt/output/model.onnx /mnt/result/b587b9de-04d7-46d1-842c-83ef709d73a2\n",
      "cpu_openmp_OMP_WAIT_POLICY_active 0.039523\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "OMP_NUM_THREADS=7\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -o 99 -m times -r 200 /mnt/output/model.onnx /mnt/result/2e778e65-f72b-4503-ae0a-e7c514f2fd27\n",
      "dnnl_7_OMP_threads_OMP_WAIT_POLICY_active 0.035018\n",
      "\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/cpu/onnxruntime_perf_test -e cpu -x 7 -o 99 -m times -r 200 /mnt/output/model.onnx /mnt/result/572acbea-3537-4833-99ec-ae3ad6681ab3\n",
      "cpu_7_intra_threads 0.036882\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "OMP_NUM_THREADS=10\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e openvino -o 99 -m times -r 200 /mnt/output/model.onnx /mnt/result/14f8b398-8632-44c0-9e87-6edb458da88c\n",
      "openvino_10_OMP_threads_OMP_WAIT_POLICY_active 0.259191\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "OMP_NUM_THREADS=7\n",
      "/perf_tuning/src/../bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e nuphar -o 99 -m times -r 200 /mnt/output/model.onnx /mnt/result/fee60ff5-ab97-4353-a145-6f9c00cb0f6e\n",
      "nuphar_7_OMP_threads_OMP_WAIT_POLICY_active 0.032309\n",
      "\n",
      "Results:\n",
      "nuphar_7_OMP_threads_OMP_WAIT_POLICY_active 0.032309 ms\n",
      "nuphar_2_OMP_threads_OMP_WAIT_POLICY_active 0.034190000000000005 ms\n",
      "nuphar_10_OMP_threads_OMP_WAIT_POLICY_active 0.03724 ms\n",
      "dnnl_7_OMP_threads_OMP_WAIT_POLICY_active 0.035018 ms\n",
      "dnnl_2_OMP_threads_OMP_WAIT_POLICY_active 0.039814999999999996 ms\n",
      "dnnl_8_OMP_threads_OMP_WAIT_POLICY_active 0.04264 ms\n",
      "cpu_7_intra_threads 0.036882 ms\n",
      "cpu 0.031565 ms\n",
      "cpu_8_intra_threads 0.03592 ms\n",
      "cpu_openmp_OMP_WAIT_POLICY_active 0.039523 ms\n",
      "cpu_openmp_3_OMP_threads_OMP_WAIT_POLICY_active 0.030414999999999998 ms\n",
      "cpu_openmp_2_OMP_threads_OMP_WAIT_POLICY_active 0.030485 ms\n",
      "openvino_10_OMP_threads_OMP_WAIT_POLICY_active 0.259191 ms\n",
      "openvino_11_OMP_threads_OMP_WAIT_POLICY_active 0.14663 ms\n",
      "openvino_OMP_WAIT_POLICY_active 0.18883999999999998 ms\n"
     ]
    }
   ],
   "source": [
    "!docker run -v {shared_dir}:/mnt \\\n",
    "    mcr.microsoft.com/onnxruntime/perf-tuning \\\n",
    "    --model /mnt/{onnx_model} \\\n",
    "    --result /mnt/{result_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check your results\n",
    "\n",
    "Besides prints from the docker run, a couple of files are stored to the result folder you specified:\n",
    "\n",
    " - `latencies.txt` has a brief summary of the best results and settings for each execution providers. \n",
    " - `latencies.json` has more detailed information such as python code snippets of how to reproduce the performance results, p90 and p95 performance numbers, on the those good combinations. \n",
    " - `profile_[ep].json` files are profiling files for the best setting from each execution providers. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
