{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License.\n",
    "\n",
    "# Convert Models and Tune Performance with OLive Docker Images\n",
    "\n",
    "This notebook demos how to use OLive Docker images `onnx-converter` and `perf-tuning` to convert a model from another model framework to ONNX, and then tune performance for the converted ONNX model.\n",
    "\n",
    "# 0. Prerequisites\n",
    "\n",
    "1) Make sure you have [Docker](https://www.docker.com/get-started) installed and running. \n",
    "\n",
    "2) Familiar with basic Docker terminologies and commands such as volumes.\n",
    "\n",
    "3) Pull the latest `onnx-converter` and `perf-tuning` docker images from MCR. This should take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\n",
      "latest: Pulling from onnxruntime/onnx-converter\n",
      "Digest: sha256:37479e01a7c4cd2e77c012f0fc3bb4e89e1b45b72c0d4bb22621286c5c02aa26\n",
      "Status: Image is up to date for mcr.microsoft.com/onnxruntime/onnx-converter:latest\n",
      "mcr.microsoft.com/onnxruntime/onnx-converter:latest\n",
      "Using default tag: latest\n",
      "latest: Pulling from onnxruntime/perf-tuning\n",
      "Digest: sha256:8003f5ecd2e11c2fdad31610294982404954570c4ced0caed7b9b6f268e9388f\n",
      "Status: Image is up to date for mcr.microsoft.com/onnxruntime/perf-tuning:latest\n",
      "mcr.microsoft.com/onnxruntime/perf-tuning:latest\n"
     ]
    }
   ],
   "source": [
    "# pull onnx-converter and perf-tuning docker images from mcr\n",
    "!docker pull mcr.microsoft.com/onnxruntime/onnx-converter\n",
    "!docker pull mcr.microsoft.com/onnxruntime/perf-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Install dependencies to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (3.2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 19.0.3, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: netron in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: onnx in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from onnx) (3.11.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from onnx) (3.7.4.1)\n",
      "Requirement already satisfied: six in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from onnx) (1.12.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from onnx) (1.17.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ziyl.northamerica\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from protobuf->onnx) (46.1.3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install wget netron onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convert Model To ONNX\n",
    "\n",
    "The first step in OLive is to convert a model of your choice to ONNX using `onnx-converter` Docker image. The Docker image first converts the model to ONNX, then tries to generate input test data if none provided, and finally runs the converted model alongside the original model for correctness check. \n",
    "\n",
    "### Prepare model and test data files\n",
    "\n",
    "First you'll need to prepare your model and test data files. Supported model frameworks are - cntk, coreml, keras, scikit-learn, tensorflow and pytorch. \n",
    "\n",
    "For `onnx-converter`, test data are used for a quick verification of correctness for the converted model. We strongly recommend you provide your own test input and output files. However if no input files are provided, OLive will randomly generate dummy inputs for you if possible. Only one test data set is needed, but feel free to put in more data sets as they will be available for the next `perf-tuning` step to use. \n",
    "\n",
    "You can put your test data in one of - \n",
    "\n",
    "  1) Your input folder with your model from another framework.\n",
    "  \n",
    "  2) Your output folder created in advance to hold your converted ONNX model.\n",
    "  \n",
    "  3) Any other location. Need to specify the path with `--test_data_path` parameter to `onnx-converter`.\n",
    "  \n",
    "The best practice to put your input model file(s) and test data(optional) is **2)**. By putting test_data_sets in the \"output\" folder instead of the \"input\" folder, this approach avoids copying files in the backend. The folder structure will be as below:\n",
    "\n",
    "    - your_input_folder\n",
    "       - model_file(s)\n",
    "    - your_output_folder_to_hold_onnx_file\n",
    "       - test_data_set_0\n",
    "           - input_0.pb\n",
    "           - ...\n",
    "           - output_0.pb\n",
    "           - ...\n",
    "       - test_data_set_1\n",
    "           - ...\n",
    "       ...\n",
    "       - (your .onnx file after running \"onnx-converter\")\n",
    "\n",
    "\n",
    "\n",
    "#### [OPTIONAL] Convert Test Data to ONNX pb \n",
    "\n",
    "ONNX .pb files are expected for test data. If you're more familiar with pickle files, we provide a convenient script to convert your pickle data to pb. Dump your input data to a single pickle file in the following dict format - \n",
    "\n",
    "    {\n",
    "        \"input_name_0\": input_data_0,\n",
    "        \"input_name_1\": input_data_1, \n",
    "        ...\n",
    "    }\n",
    "    \n",
    "or if dumping output data - \n",
    "\n",
    "    {\n",
    "        \"output_name_0\": output_data_0,\n",
    "        \"output_name_1\": output_data_1, \n",
    "        ...\n",
    "    }\n",
    "\n",
    "Then use the [convert_test_data.py](https://github.com/microsoft/OLive/blob/master/utils/convert_test_data.py) to convert your pickle file to pb files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0% [                                                                                ]    0 / 2694\r",
      "100% [................................................................................] 2694 / 2694Downloaded scripts\\convert_test_data.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wget\n",
    "\n",
    "# Download convert_test_data.py\n",
    "url = \"https://raw.githubusercontent.com/microsoft/OLive/master/utils/convert_test_data.py\"\n",
    "script_dir = \"scripts\"\n",
    "if not os.path.exists(script_dir):\n",
    "    os.makedirs(script_dir)\n",
    "\n",
    "script_file = os.path.join(script_dir, 'convert_test_data.py')\n",
    "if os.path.exists(script_file):\n",
    "    os.remove(script_file)\n",
    "wget.download(url, script_file)\n",
    "print(\"Downloaded\", script_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run convert_test_data.py to convert your pickle file. This script will read your pickle file and dump the data to a folder named \"test_data_set_0\" by default. Note that ONNX naming convention for test data folder is \"test_data_*\". Make sure to pass `--output_folder` with a folder name starting with `test_data_`. \n",
    "\n",
    "If `--is_input=True`, data will be generated to `input_*.pb`s. Set `--is_input` to false if you'd like to generate output pbs, in which data will be generated to `output_*.pb`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./scripts/convert_test_data.py <your_input_pickle_file> --output_folder <output_folder (/test_data_set_0)> --is_input=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you're ready to use OLive to convert model and tune performance. In this tutorial, we'll use [MNIST model from ONNX model zoo](https://github.com/onnx/models/tree/master/vision/classification/mnist) as an example to demo the OLive pipeline. Below are some code to download the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully downloaded and extracted in  test_models\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Download and store the model to the desire directory. Modify these lines to point to your local model directory \n",
    "model_url = \"https://onnxzoo.blob.core.windows.net/models/opset_8/mnist/mnist.tar.gz\"\n",
    "model_dir = \"test_models\"\n",
    "absolute_model_dir = os.path.abspath(model_dir)\n",
    "if not os.path.exists(model_dir):\n",
    "    print(\"Creating model directory \", model_dir)\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "import tarfile\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "file_tmp = urlretrieve(model_url, filename=None)[0]\n",
    "\n",
    "tar = tarfile.open(file_tmp)\n",
    "tar.extractall(model_dir)\n",
    "print(\"Model successfully downloaded and extracted in \", model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run `onnx-converter` Docker Image\n",
    "\n",
    "For the docker image to access your model and test data, you'll need to mount a local directory to docker using the docker parameter `-v`. By using `-v <your_local_directory>:/mnt`, you will be able to share all your files under `<your_local_directory>` to the `/mnt` folder in a running Docker container. Note `your_local_directory` has to be an absolute path. Detailed concept and usage are explained [here](https://docs.docker.com/engine/reference/run/#volume-shared-filesystems)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder shared with docker is  C:\\Users\\ziyl.NORTHAMERICA\\OLive\\notebook\\test_models\n",
      "Converted ONNX model will be stored at  C:\\Users\\ziyl.NORTHAMERICA\\OLive\\notebook\\test_models\\output/model.onnx\n"
     ]
    }
   ],
   "source": [
    "# Get the absolute model dir for the local directory to share\n",
    "shared_dir = os.path.abspath(model_dir)\n",
    "# Your input model relative to absolute_model_dir\n",
    "input_model = \"mnist/model.onnx\"\n",
    "# Specify the output folder and converted model name (relative to shared_dir)\n",
    "output_onnx_path = \"output/model.onnx\"\n",
    "# Change model_type to tensorflow, pytorchcntk, coreml, keras or scikit-learn\n",
    "model_type = \"onnx\"\n",
    "print(\"Folder shared with docker is \", shared_dir)\n",
    "print(\"Converted ONNX model will be stored at \", os.path.join(shared_dir, output_onnx_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For different model framework, different parameters are needed for the `onnx-converter` to run. Detailed information on what parameters are needed for your specific model framework, please check onnx-converter [README.md](https://github.com/microsoft/OLive/blob/master/docker-images/onnx-converter/README.md).\n",
    "\n",
    "You can also add a `--test_data_path` parameter to specify your own test data folder (the parent folder to your \"test_data_\\*\" folders) if your test data lie in neither the directory of your input model nor your `--output_onnx_path`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------\n",
      "Model Conversion\n",
      "\n",
      "Input model is already ONNX model. Skipping conversion.\n",
      "\n",
      "-------------\n",
      "MODEL INPUT GENERATION(if needed)\n",
      "\n",
      "Test data .pb files found under /mnt/mnist/test_data_set_0. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:TensorFlow version 1.15.0 detected. Last version known to be fully compatible is 1.14.0 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copying /mnt/mnist/test_data_set_0 to /mnt/output/test_data_set_0\n",
      "Test data .pb files found under /mnt/mnist/test_data_set_1. \n",
      "copying /mnt/mnist/test_data_set_1 to /mnt/output/test_data_set_1\n",
      "Test data .pb files found under /mnt/mnist/test_data_set_2. \n",
      "copying /mnt/mnist/test_data_set_2 to /mnt/output/test_data_set_2\n",
      "Test data .pb files already exist. Skipping dummy input generation. \n",
      "\n",
      "-------------\n",
      "MODEL CORRECTNESS VERIFICATION\n",
      "\n",
      "\n",
      "Check the ONNX model for validity \n",
      "The ONNX model is valid.\n",
      "\n",
      "The original model is already onnx. Skipping correctness test. \n",
      "\n",
      "-------------\n",
      "MODEL CONVERSION SUMMARY (.json file generated at /mnt/output/output.json )\n",
      "\n",
      "{'conversion_status': 'SUCCESS',\n",
      " 'correctness_verified': 'SKIPPED',\n",
      " 'error_message': '',\n",
      " 'input_folder': '/mnt/output/test_data_set_0',\n",
      " 'output_onnx_path': '/mnt/output/model.onnx'}\n"
     ]
    }
   ],
   "source": [
    "!docker run -v {shared_dir}:/mnt \\\n",
    "    mcr.microsoft.com/onnxruntime/onnx-converter \\\n",
    "    --model /mnt/{input_model} \\\n",
    "    --output_onnx_path /mnt/{output_onnx_path} \\\n",
    "    --model_type {model_type}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the converted ONNX model\n",
    "\n",
    "By using Netron model visualization tool, we can check out the newly converted ONNX model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'C:\\Users\\ziyl.NORTHAMERICA\\OLive\\notebook\\test_models\\output/model.onnx' at http://localhost:8080\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"1000\"\n",
       "            src=\"http://localhost:8080\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x224ba858438>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import netron\n",
    "netron.start(os.path.join(shared_dir, output_onnx_path), browse=False)\n",
    "from IPython.display import IFrame\n",
    "IFrame('http://localhost:8080', width=\"100%\", height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopping http://localhost:8080\n"
     ]
    }
   ],
   "source": [
    "# Stop Netron server\n",
    "netron.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your model has been successfully converted. The model file and its test data are stored in the `output_onnx_path` folder. The files in this folder are ready to serve as inputs for the following `perf-tuning` step. \n",
    "\n",
    "## 2. Performance Tuning\n",
    "\n",
    "The `perf-tuning` docker image tunes the best settings for an ONNX model to run in ONNX Runtime. It sweeps combinations of threads, environment variables, and execution providers for the best performance numbers. Top 3 results will be rendered for each available execution provider.  \n",
    "\n",
    "### Prepare inputs\n",
    "\n",
    "You'll need to store your ONNX model file as well as its test data in the same folder in the following structure - \n",
    "\n",
    "    -- local_directory_to_your_models\n",
    "        -- ModelDir\n",
    "            -- model.onnx\n",
    "            -- test_data_set_0\n",
    "                -- input_0.pb\n",
    "                -- output_0.pb\n",
    "                -- ...\n",
    "            -- test_data_set_1            \n",
    "                -- ...\n",
    "            -- ...\n",
    "            \n",
    "Test data is required in this step. If you follow the conversion step in the notebook to this point, the output folder for the `onnx-converter` is already in this folder structure and can be directly used as input for `perf-tuning`.\n",
    "\n",
    "### Run `perf-tuning` Docker Image\n",
    "\n",
    "\n",
    "A few things to note when running the `perf-tuning` Docker image: \n",
    "\n",
    " - Currently supported execution providers(EPs) are cpu, cpu_openmp, dnnl, mklml, cuda, tensorrt, ngraph, and nuphar. Add `-e cpu,dnnl,...`(no spaces between the EPs) to select the execution providers you'd like to tune. By default, all available EPs will be tuned.\n",
    " - EPs such as CUDA and TensorRT require GPU. To use those EPs, make sure you have GPU in your local machine, and add `--gpus all` to Docker (BEFORE `mcr.microsoft.com/onnxruntime/perf-tuning`) to leverage your GPUs. Otherwise GPU based execution providers will be skipped. \n",
    " - Just like `onnx-converter`, `perf-tuning` also needs users to share their local directories to the Docker container using the `-v` command. \n",
    " - Other available commands for `perf-tuning` are documented [here](https://github.com/microsoft/OLive/tree/master/docker-images/perf-tuning). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some variables \n",
    "onnx_model = output_onnx_path        # reuse output of the \"convert\" step here. Adjust as neccessary\n",
    "result_dir = \"result\"             # output folder to hold your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting intra_op_num_threads to 1\n",
      "Session creation time cost:0.0081595 s\n",
      "Total inference time cost:0.0034581 s\n",
      "Total inference requests:20\n",
      "Average inference time cost:0.172905 ms\n",
      "Total inference run time:0.003502 s\n",
      "Setting intra_op_num_threads to 1\n",
      "Session creation time cost:0.0045487 s\n",
      "Total inference time cost:0.007971 s\n",
      "Total inference requests:20\n",
      "Average inference time cost:0.39855 ms\n",
      "Total inference run time:0.0080087 s\n",
      "Setting intra_op_num_threads to 1\n",
      "Session creation time cost:0.0050911 s\n",
      "Total inference time cost:0.0017617 s\n",
      "Total inference requests:20\n",
      "Average inference time cost:0.088085 ms\n",
      "Total inference run time:0.0017969 s\n",
      "Setting intra_op_num_threads to 1\n",
      "Session creation time cost:0.0046958 s\n",
      "Total inference time cost:0.0013464 s\n",
      "Total inference requests:20\n",
      "Average inference time cost:0.06732 ms\n",
      "Total inference run time:0.001369 s\n",
      "Setting intra_op_num_threads to 1\n",
      "Session creation time cost:0.0041199 s\n",
      "Total inference time cost:0.0091266 s\n",
      "Total inference requests:20\n",
      "Average inference time cost:0.45633 ms\n",
      "Total inference run time:0.0091695 s\n",
      "Setting intra_op_num_threads to 1\n",
      "Session creation time cost:0.0058266 s\n",
      "Total inference time cost:0.0015766 s\n",
      "Total inference requests:20\n",
      "Average inference time cost:0.07883 ms\n",
      "Total inference run time:0.0016138 s\n",
      "Setting intra_op_num_threads to 1\n",
      "Session creation time cost:0.0046164 s\n",
      "Total inference time cost:0.0011242 s\n",
      "Total inference requests:20\n",
      "Average inference time cost:0.0562101 ms\n",
      "Total inference run time:0.0011426 s\n",
      "Setting intra_op_num_threads to 1\n",
      "Session creation time cost:0.0038202 s\n",
      "Total inference time cost:0.0014446 s\n",
      "Total inference requests:20\n",
      "Average inference time cost:0.07223 ms\n",
      "Total inference run time:0.0014732 s\n",
      "Setting intra_op_num_threads to 1\n",
      "Session creation time cost:0.0045246 s\n",
      "Total inference time cost:0.0012797 s\n",
      "Total inference requests:20\n",
      "Average inference time cost:0.063985 ms\n",
      "Total inference run time:0.0013025 s\n",
      "Setting intra_op_num_threads to 1\n",
      "Session creation time cost:0.0044264 s\n",
      "Total inference time cost:0.0011836 s\n",
      "Total inference requests:20\n",
      "Average inference time cost:0.05918 ms\n",
      "Total inference run time:0.0012052 s\n",
      "Setting intra_op_num_threads to 1\n",
      "Session creation time cost:0.0202724 s\n",
      "Total inference time cost:0.0016248 s\n",
      "Total inference requests:20\n",
      "Average inference time cost:0.08124 ms\n",
      "Total inference run time:0.0016466 s\n",
      "Setting intra_op_num_threads to 1\n",
      "Session creation time cost:0.0041055 s\n",
      "Total inference time cost:0.0097834 s\n",
      "Total inference requests:20\n",
      "Average inference time cost:0.48917 ms\n",
      "Total inference run time:0.0098162 s\n",
      "Setting intra_op_num_threads to 1\n",
      "Session creation time cost:0.0050472 s\n",
      "Total inference time cost:0.001256 s\n",
      "Total inference requests:20\n",
      "Average inference time cost:0.0628 ms\n",
      "Total inference run time:0.0012744 s\n",
      "Setting intra_op_num_threads to 1\n",
      "Session creation time cost:0.0064559 s\n",
      "Total inference time cost:0.0011311 s\n",
      "Total inference requests:20\n",
      "Average inference time cost:0.056555 ms\n",
      "Total inference run time:0.0011496 s\n",
      "Setting intra_op_num_threads to 1\n",
      "Session creation time cost:0.0043139 s\n",
      "Total inference time cost:0.0083875 s\n",
      "Total inference requests:20\n",
      "Average inference time cost:0.419375 ms\n",
      "Total inference run time:0.0084179 s\n",
      "Setting intra_op_num_threads to 0\n",
      "Session creation time cost:0.0039241 s\n",
      "Total inference time cost:0.0011575 s\n",
      "Total inference requests:20\n",
      "Average inference time cost:0.057875 ms\n",
      "Total inference run time:0.0011771 s\n",
      "Setting intra_op_num_threads to 0\n",
      "Setting intra_op_num_threads to 0\n",
      "Setting intra_op_num_threads to 1\n",
      "Session creation time cost:0.0952866 s\n",
      "Total inference time cost:0.0041222 s\n",
      "Total inference requests:20\n",
      "Average inference time cost:0.20611 ms\n",
      "Total inference run time:0.004164 s\n",
      "Setting intra_op_num_threads to 1\n",
      "Session creation time cost:0.0948249 s\n",
      "Total inference time cost:0.0014944 s\n",
      "Total inference requests:20\n",
      "Average inference time cost:0.07472 ms\n",
      "Total inference run time:0.0015134 s\n",
      "Setting intra_op_num_threads to 1\n",
      "Session creation time cost:0.103111 s\n",
      "Total inference time cost:0.0016686 s\n",
      "Total inference requests:20\n",
      "Average inference time cost:0.08343 ms\n",
      "Total inference run time:0.0016899 s\n",
      "Setting intra_op_num_threads to 1\n",
      "Session creation time cost:0.0915553 s\n",
      "Total inference time cost:0.0015635 s\n",
      "Total inference requests:20\n",
      "Average inference time cost:0.078175 ms\n",
      "Total inference run time:0.0015849 s\n",
      "Setting intra_op_num_threads to 1\n",
      "Session creation time cost:0.0977587 s\n",
      "Total inference time cost:0.0015738 s\n",
      "Total inference requests:20\n",
      "Average inference time cost:0.07869 ms\n",
      "Total inference run time:0.0015941 s\n",
      "Setting intra_op_num_threads to 1\n",
      "Session creation time cost:0.0042567 s\n",
      "Total inference time cost:0.0101774 s\n",
      "Total inference requests:200\n",
      "Average inference time cost:0.050887 ms\n",
      "Total inference run time:0.0103399 s\n",
      "Setting intra_op_num_threads to 1\n",
      "Session creation time cost:0.0041876 s\n",
      "Total inference time cost:0.0126329 s\n",
      "Total inference requests:200\n",
      "Average inference time cost:0.0631645 ms\n",
      "Total inference run time:0.0127952 s\n",
      "Setting intra_op_num_threads to 1\n",
      "Session creation time cost:0.0041092 s\n",
      "Total inference time cost:0.011132 s\n",
      "Total inference requests:200\n",
      "Average inference time cost:0.05566 ms\n",
      "Total inference run time:0.0112866 s\n",
      "Setting intra_op_num_threads to 0\n",
      "Session creation time cost:0.0039692 s\n",
      "Total inference time cost:0.0112665 s\n",
      "Total inference requests:200\n",
      "Average inference time cost:0.0563325 ms\n",
      "Total inference run time:0.0114702 s\n",
      "Setting intra_op_num_threads to 1\n",
      "Session creation time cost:0.104932 s\n",
      "Total inference time cost:0.0161573 s\n",
      "Total inference requests:200\n",
      "Average inference time cost:0.0807865 ms\n",
      "Total inference run time:0.0163919 s\n",
      "Cores:  2\n",
      "No GPU found on current device. Cuda and TensorRT performance tuning might not be available. \n",
      "providers  ['cpu_openmp', 'mklml', 'dnnl', 'cpu', 'tensorrt', 'ngraph', 'cuda', 'nuphar']\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "OMP_NUM_THREADS=2\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -x 1 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/dfa10e0e-d05e-495f-885d-5136ab831b76\n",
      "cpu_openmp_2_OMP_threads_OMP_WAIT_POLICY_active 0.172905\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "OMP_NUM_THREADS=2\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -x 1 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/7c28215d-a226-459f-8b1d-fbb2ac8784fe\n",
      "cpu_openmp_2_OMP_threads_OMP_WAIT_POLICY_passive 0.39855\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -x 1 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/e3f5c3dc-f196-4b88-bbb3-406e4e872f4f\n",
      "cpu_openmp 0.088085\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -x 1 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/1029964e-e9c9-4166-8403-045488e157a1\n",
      "cpu_openmp_OMP_WAIT_POLICY_active 0.06732\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -x 1 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/7c0c1eab-c63b-4d42-bf30-cdadab0b3758\n",
      "cpu_openmp_OMP_WAIT_POLICY_passive 0.45633\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "OMP_NUM_THREADS=2\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -x 1 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/1ad5b06e-1d03-4c3a-9a12-d7cd55c371bb\n",
      "mklml_2_OMP_threads_OMP_WAIT_POLICY_active 0.07883\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "OMP_NUM_THREADS=2\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -x 1 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/0a9b8757-2524-468d-a449-13aae1f3775b\n",
      "mklml_2_OMP_threads_OMP_WAIT_POLICY_passive 0.056209999999999996\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -x 1 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/e8088011-960f-4a7f-89f4-b0799e875214\n",
      "mklml 0.07223\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -x 1 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/0aebe18f-32a7-478b-b5de-9f3d960d61e3\n",
      "mklml_OMP_WAIT_POLICY_active 0.063985\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -x 1 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/efcd7d94-cae1-4827-902d-c45bd398d690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyl/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:107 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] /home/ziyl/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:101 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] CUDA failure 35: CUDA driver version is insufficient for CUDA runtime version ; GPU=32722 ; hostname=f923670d0d86 ; expr=cudaSetDevice(device_id_); \n",
      "\n",
      "\n",
      "/home/ziyl/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:107 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] /home/ziyl/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:101 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] CUDA failure 35: CUDA driver version is insufficient for CUDA runtime version ; GPU=32645 ; hostname=f923670d0d86 ; expr=cudaSetDevice(device_id_); \n",
      "\n",
      "\n",
      "[00:14:16] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "[00:14:16] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "[00:14:16] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=32\n",
      "[00:14:16] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "[00:14:16] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "[00:14:16] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=32\n",
      "[00:14:17] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "[00:14:17] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "[00:14:17] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=32\n",
      "[00:14:17] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "[00:14:17] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "[00:14:17] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=32\n",
      "[00:14:17] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "[00:14:17] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "[00:14:17] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=32\n",
      "[00:14:20] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "[00:14:20] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=1\n",
      "[00:14:20] /home/ziyl/onnxruntime/cmake/external/tvm/src/pass/arg_binder.cc:92: Trying to bind buffer to another one with lower alignment requirement  required_alignment=64, provided_alignment=32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mklml_OMP_WAIT_POLICY_passive 0.059179999999999996\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "OMP_NUM_THREADS=2\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -x 1 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/f0ace92b-7170-4bc4-ba7b-ead0dc018af2\n",
      "dnnl_2_OMP_threads_OMP_WAIT_POLICY_active 0.08124\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "OMP_NUM_THREADS=2\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -x 1 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/48a4edcf-6eef-45f0-b22a-88b4dd176f12\n",
      "dnnl_2_OMP_threads_OMP_WAIT_POLICY_passive 0.48917000000000005\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -x 1 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/286f89f6-8760-47ee-823d-fdcd0c332e4f\n",
      "dnnl 0.0628\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -x 1 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/36651ee5-fea9-4212-89ee-2944709e636d\n",
      "dnnl_OMP_WAIT_POLICY_active 0.056555\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -x 1 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/250ce768-3146-4843-9806-db71b60f7b38\n",
      "dnnl_OMP_WAIT_POLICY_passive 0.419375\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e cpu -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/2c9dd0f0-a376-4274-bfaa-42988cdb6a73\n",
      "cpu 0.057874999999999996\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e tensorrt -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/f575b42b-d0da-4e00-a87a-b61637394181\n",
      "tensorrt None\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "OMP_NUM_THREADS=2\n",
      "/perf_tuning/bin/RelWithDebInfo/ngraph/onnxruntime_perf_test -e ngraph -x 1 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/45a6a549-0454-4293-939d-81376ce29ae6\n",
      "ngraph_2_OMP_threads_OMP_WAIT_POLICY_active None\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "OMP_NUM_THREADS=2\n",
      "/perf_tuning/bin/RelWithDebInfo/ngraph/onnxruntime_perf_test -e ngraph -x 1 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/7f22faf5-0b8d-4347-bf0f-a2b179680a04\n",
      "ngraph_2_OMP_threads_OMP_WAIT_POLICY_passive None\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/ngraph/onnxruntime_perf_test -e ngraph -x 1 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/0e3566e3-cbbb-4e73-8c9c-f0d04c7a2d13\n",
      "ngraph None\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "/perf_tuning/bin/RelWithDebInfo/ngraph/onnxruntime_perf_test -e ngraph -x 1 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/cadfd7c2-ce69-483e-b536-c70dd4ab9790\n",
      "ngraph_OMP_WAIT_POLICY_active None\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "/perf_tuning/bin/RelWithDebInfo/ngraph/onnxruntime_perf_test -e ngraph -x 1 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/9ded94cd-156a-44dc-b781-949339c21bae\n",
      "ngraph_OMP_WAIT_POLICY_passive None\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e cuda -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/946e1fc8-e747-422a-815e-35b025afb5f1\n",
      "cuda None\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "OMP_NUM_THREADS=2\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -e nuphar -x 1 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/a0ac5b56-0afa-47df-b453-0df5119b5572\n",
      "nuphar_2_OMP_threads_OMP_WAIT_POLICY_active 0.20611\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "OMP_NUM_THREADS=2\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -e nuphar -x 1 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/cd64556c-6150-46a2-9748-d684e3332c13\n",
      "nuphar_2_OMP_threads_OMP_WAIT_POLICY_passive 0.07472000000000001\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -e nuphar -x 1 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/ee109a7f-9492-4d42-a9ca-7e8a256e009b\n",
      "nuphar 0.08343\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -e nuphar -x 1 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/6c8ce722-60e4-489a-b2d7-db242afeb196\n",
      "nuphar_OMP_WAIT_POLICY_active 0.07817500000000001\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -e nuphar -x 1 -o 99 -m times -r 20 /mnt/output/model.onnx /mnt/result/f5d0b0cd-3656-4976-960c-94903eceae1f\n",
      "nuphar_OMP_WAIT_POLICY_passive 0.07869\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -x 1 -o 99 -m times -r 200 /mnt/output/model.onnx /mnt/result/9261255a-c0fa-445a-bd6d-6657c9c05105\n",
      "cpu_openmp_OMP_WAIT_POLICY_active 0.050887\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "OMP_NUM_THREADS=2\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -x 1 -o 99 -m times -r 200 /mnt/output/model.onnx /mnt/result/f5b893bc-8732-42c8-a83e-77b14f32ac30\n",
      "mklml_2_OMP_threads_OMP_WAIT_POLICY_passive 0.063165\n",
      "\n",
      "OMP_WAIT_POLICY=active\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e dnnl -x 1 -o 99 -m times -r 200 /mnt/output/model.onnx /mnt/result/20ac2e31-770e-4037-adaa-9fdbd3f56977\n",
      "dnnl_OMP_WAIT_POLICY_active 0.05566\n",
      "\n",
      "/perf_tuning/bin/RelWithDebInfo/all_eps/onnxruntime_perf_test -e cpu -o 99 -m times -r 200 /mnt/output/model.onnx /mnt/result/57ef1783-cabe-4f2c-a6b3-79feee4066e9\n",
      "cpu 0.056332999999999994\n",
      "\n",
      "OMP_WAIT_POLICY=passive\n",
      "OMP_NUM_THREADS=2\n",
      "/perf_tuning/bin/RelWithDebInfo/mklml/onnxruntime_perf_test -e nuphar -x 1 -o 99 -m times -r 200 /mnt/output/model.onnx /mnt/result/bf6414b3-d998-4a5d-ac08-636b0e24ff66\n",
      "nuphar_2_OMP_threads_OMP_WAIT_POLICY_passive 0.080787\n",
      "\n",
      "Results:\n",
      "cpu_openmp_OMP_WAIT_POLICY_active 0.050887 ms\n",
      "cpu_openmp 0.088085 ms\n",
      "cpu_openmp_2_OMP_threads_OMP_WAIT_POLICY_active 0.172905 ms\n",
      "dnnl_OMP_WAIT_POLICY_active 0.05566 ms\n",
      "dnnl 0.0628 ms\n",
      "dnnl_2_OMP_threads_OMP_WAIT_POLICY_active 0.08124 ms\n",
      "cpu 0.056332999999999994 ms\n",
      "mklml_2_OMP_threads_OMP_WAIT_POLICY_passive 0.063165 ms\n",
      "mklml_OMP_WAIT_POLICY_passive 0.059179999999999996 ms\n",
      "mklml_OMP_WAIT_POLICY_active 0.063985 ms\n",
      "nuphar_2_OMP_threads_OMP_WAIT_POLICY_passive 0.080787 ms\n",
      "nuphar_OMP_WAIT_POLICY_active 0.07817500000000001 ms\n",
      "nuphar_OMP_WAIT_POLICY_passive 0.07869 ms\n",
      "ngraph_2_OMP_threads_OMP_WAIT_POLICY_active error\n",
      "ngraph_2_OMP_threads_OMP_WAIT_POLICY_passive error\n",
      "ngraph error\n",
      "ngraph_OMP_WAIT_POLICY_active error\n",
      "ngraph_OMP_WAIT_POLICY_passive error\n"
     ]
    }
   ],
   "source": [
    "!docker run -v {shared_dir}:/mnt \\\n",
    "    mcr.microsoft.com/onnxruntime/perf-tuning \\\n",
    "    --model /mnt/{onnx_model} \\\n",
    "    --result /mnt/{result_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check your results\n",
    "\n",
    "Besides prints from the docker run, a couple of files are stored to the result folder you specified:\n",
    "\n",
    " - `latencies.txt` has a brief summary of the best results and settings for each execution providers. \n",
    " - `latencies.json` has more detailed information such as python code snippets of how to reproduce the performance results, p90 and p95 performance numbers, on the those good combinations. \n",
    " - `profile_[ep].json` files are profiling files for the best setting from each execution providers. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
