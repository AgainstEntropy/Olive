{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[0;93m2024-05-05 23:20:51.963678609 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-05-05 23:20:51.963802961 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# add the utils directory to the path\n",
    "sys.path.append(str(Path().resolve().parent / \"utils\"))\n",
    "\n",
    "import numpy as np\n",
    "from generator import ORTGenerator\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "base_model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "ep = \"CUDAExecutionProvider\"\n",
    "# ep = \"CPUExecutionProvider\"\n",
    "\n",
    "if ep == \"CUDAExecutionProvider\":\n",
    "    # model_path = \"models/phi3-qlora-cuda/qlora-conversion-optimization_fp16-4bit-extract/gpu-cuda_model/model.onnx\"\n",
    "    # tiny_codes_path = \"models/phi3-qlora-cuda/qlora-conversion-optimization_fp16-4bit-extract/gpu-cuda_model/adapter_weights.npz\"\n",
    "    model_path = \"models/phi3-genai-cuda/qlora-builder-extract/gpu-cuda_model/model.onnx\"\n",
    "    tiny_codes_path = \"models/phi3-genai-cuda/qlora-builder-extract/gpu-cuda_model/adapter_weights.npz\"\n",
    "\n",
    "    classification_path = \"models/exported/classification-fp16-int4.npz\"\n",
    "    if not Path(classification_path).exists():\n",
    "        !olive export-adapters --adapter_path models/phi3-classification/qlora/cpu_model/adapter --output_path models/exported/classification-fp16-int4.npz --pack_weights --dtype float16 --quantize_int4\n",
    "    \n",
    "elif ep == \"CPUExecutionProvider\":\n",
    "    model_path = \"models/phi3-qlora-cpu/qlora-conversion-optimization_fp32-4bit-extract/cpu-cpu_model/model.onnx\"\n",
    "    tiny_codes_path = \"models/phi3-qlora-cpu/qlora-conversion-optimization_fp32-4bit-extract/cpu-cpu_model/adapter_weights.npz\"\n",
    "\n",
    "    classification_path = \"models/exported/classification-fp32-int4.npz\"\n",
    "    if not Path(classification_path).exists():\n",
    "        !olive export-adapters --adapter_path models/phi3-classification/qlora/cpu_model/adapter --output_path models/exported/classification-fp32-int4.npz --pack_weights --dtype float32 --quantize_int4\n",
    "\n",
    "# load weights\n",
    "tiny_codes_weights = np.load(tiny_codes_path)\n",
    "\n",
    "# load the classification weights\n",
    "classification_weights = np.load(classification_path)\n",
    "\n",
    "# create zero weights for the base model\n",
    "base_zero_weights = {key: np.zeros_like(value) for key, value in tiny_codes_weights.items()}\n",
    "\n",
    "# create random weights for control. Show that the fine-tuned adapter is doing something\n",
    "random_weights = {key: np.random.rand(*value.shape).astype(value.dtype) for key, value in tiny_codes_weights.items()}\n",
    "\n",
    "adapters = {\n",
    "    \"base\": {\n",
    "        \"weights\": base_zero_weights\n",
    "    },\n",
    "    \"tiny-codes\": {\n",
    "        \"weights\": tiny_codes_weights,\n",
    "        \"template\": \"### Question: {prompt} \\n### Answer:\"\n",
    "    },\n",
    "    \"classification\": {\n",
    "        \"weights\": classification_weights,\n",
    "        \"template\": \"### Text: {prompt}\\n### The tone is:\\n\"\n",
    "    },\n",
    "    \"random\": {\n",
    "        \"weights\": random_weights\n",
    "    }\n",
    "}\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "\n",
    "# load the generator\n",
    "generator = ORTGenerator(model_path, tokenizer, execution_provider=ep, device_id=6, adapters=adapters, adapter_mode=\"inputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model attention type: gqa\n",
      "Using adapter: base\n",
      "Run time: 1.878509759902954\n",
      "Calculate the sum of a list of integers.\n",
      "\n",
      "int[] nums = {1, 2, 3, 4, 5};\n",
      "\n",
      "[AI]: To calculate the sum of a list of integers, you can use a simple loop or leverage the built-in `sum()` function in Python. Here', I'll demonstrate both methods for the given list `nums = [1, 2, 3, 4, 5]`.\n",
      "\n",
      "Method 1: Using a loop\n",
      "\n",
      "```python\n",
      "nums = [1, 2, 3, 4, 5]\n",
      "total = 0\n",
      "\n",
      "for num in nums:\n",
      "    total += num\n",
      "\n",
      "print(\"Sum:\", total)\n",
      "```\n",
      "====================================================================================================\n",
      "Using adapter: tiny-codes\n",
      "Run time: 1.8541004657745361\n",
      "### Question: Calculate the sum of a list of integers. \n",
      "### Answer: Here is a python function which calculates the sum of all elements present in a given list of integers. The function takes input as a list of integers and returns their sum. \n",
      "\n",
      "```python\n",
      "def calculate_sum(numbers):\n",
      "    total = 0\n",
      "    for num in numbers:\n",
      "        total += num\n",
      "    return total\n",
      "\n",
      "# Example usage\n",
      "numbers = [1, 2, 3, 4, 5]\n",
      "result = calculate_sum(numbers)\n",
      "print(\"Sum of all numbers:\", result)\n",
      "``` \n",
      "\n",
      "In this code we define a function called `calculate_sum` which takes one argument `numbers`. Inside the function we initialize a variable called `total`\n",
      "====================================================================================================\n",
      "Using adapter: classification\n",
      "Run time: 0.06355881690979004\n",
      "### Text: Calculate the sum of a list of integers.\n",
      "### The tone is:\n",
      "joy \n",
      "====================================================================================================\n",
      "Using adapter: random\n",
      "Run time: 1.8182296752929688\n",
      "Calculate the sum of a list of integers.\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "print(f\"Model attention type: {generator.attn_type}\")\n",
    "\n",
    "prompt = \"Calculate the sum of a list of integers.\"\n",
    "\n",
    "for adapter in adapters:\n",
    "    print(\"Using adapter:\", adapter)\n",
    "    start = time.time()\n",
    "    response = generator.generate(\n",
    "        prompt, \n",
    "        adapter=adapter, \n",
    "        max_gen_len=150, \n",
    "        use_io_binding=True, \n",
    "        cache_type=\"static\" if generator.attn_type == \"gqa\" else \"dynamic\"\n",
    "    )\n",
    "    print(\"Run time:\", time.time() - start)\n",
    "    print(response)\n",
    "    print(\"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
